{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a304e42e-637d-4a03-fe66-5f9a3fd71782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Github Repo: https://github.com/parthsaxena/CS598-Final-Project\n",
        "\n",
        "# Video: https://drive.google.com/file/d/15eu31xB9dxJO9vGj80GlUjBp_ub5Cdtu/view?usp=sharing\n",
        "\n",
        "(This pdf does not show the full python notebook so please use the github repo to view the rest of it)"
      ],
      "metadata": {
        "id": "NeEnoad_hk1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Introduction\n",
        "\n",
        "## Background of the Problem\n",
        "\n",
        "### What Type of Problem\n",
        "\n",
        "Alzheimer's Disease (AD) is the most common form of dementia and poses significant challenges in diagnosis and management. The early stages, such as Mild Cognitive Impairment (MCI), often progress to Alzheimer's, making early and accurate detection critical for effective intervention. This project addresses the problem of Alzheimer’s disease diagnosis using a multimodal deep learning approach, integrating different types of medical data to improve diagnostic accuracy.\n",
        "\n",
        "### Importance and Meaning of Solving the Problem\n",
        "\n",
        "Accurately diagnosing Alzheimer's Disease is crucial as it allows for earlier intervention strategies, which can significantly delay disease progression and improve quality of life. Moreover, with the aging global population, the incidence of AD is expected to rise, increasing the burden on healthcare systems. Thus, enhancing diagnostic techniques is not only beneficial for patient care but also economically vital.\n",
        "\n",
        "### Difficulty of the Problem\n",
        "\n",
        "Diagnosing AD is inherently challenging due to its subtle onset and the overlap of symptoms with normal aging and other forms of dementia. Traditional diagnostic methods rely heavily on clinical assessments and biomarkers, which might not capture the full spectrum of the disease's progression. The integration of multiple data types (e.g., imaging, genetic, and clinical data) introduces additional complexity, including data heterogeneity and alignment, making effective analysis challenging.\n",
        "\n",
        "### State of the Art Methods and Effectiveness\n",
        "\n",
        "Current state-of-the-art methods in AD diagnosis include various machine learning models that utilize single-modality data sources, typically imaging or genetic data. While these methods have shown promise, they often fail to capture the intricate patterns across different types of data that are indicative of AD. Multimodal learning approaches have begun to address this limitation by combining information from multiple sources, leading to improved diagnostic accuracy and robustness.\n",
        "\n",
        "## Paper Explanation\n",
        "\n",
        "### Proposal of the Paper\n",
        "\n",
        "The paper proposes a novel Multimodal Alzheimer’s Disease Diagnosis framework (MADDi) that utilizes a multimodal attention-based deep learning architecture. The framework integrates imaging, genetic, and clinical data to enhance the diagnosis of Alzheimer's Disease and its precursors.\n",
        "\n",
        "### Innovations of the Method\n",
        "\n",
        "MADDi introduces a cross-modal attention mechanism that allows the model to effectively learn from and integrate multiple data modalities. This approach enables the model to capture interactions between different types of data, which is a significant advancement over traditional methods that typically concatenate features from separate modalities without exploring their interactions.\n",
        "\n",
        "### Effectiveness of the Proposed Method\n",
        "\n",
        "In the original study, MADDi achieved a state-of-the-art accuracy of 96.88% on a held-out test set for classifying control, MCI, and AD stages. This performance indicates a substantial improvement over previous models and underscores the effectiveness of leveraging cross-modal attention in multimodal learning setups.\n",
        "\n",
        "### Contribution to the Research Regime\n",
        "\n",
        "The paper’s contributions are significant as they address the critical challenge of integrating heterogeneous data types in a meaningful way. By demonstrating the effectiveness of cross-modal attention mechanisms, the study not only advances the field of Alzheimer’s diagnosis but also lays the groundwork for similar approaches in other complex, multimodal medical diagnostic tasks. This innovation opens new avenues for research into more effective, interpretable models that can better support clinical decision-making processes."
      ],
      "metadata": {
        "id": "1UGXzKlxWwWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scopes of Reproducibility\n",
        "\n",
        "### Hypothesis 1: Unimodal models provide a good baseline diagnostic accuracy.\n",
        "\n",
        "**Hypothesis Description:**\n",
        "The original paper posits that the use of cross-modal attention mechanisms significantly enhances the model’s ability to integrate and interpret data from different modalities (imaging, genetic, and clinical), leading to improved diagnostic accuracy for Alzheimer's disease and its precursor states. We will test the claim that individual biomarkers such as clinical data and imaging are sufficient for accurate models.\n",
        "\n",
        "**Experiment to Test the Hypothesis:**\n",
        "1. **Data Preparation:**\n",
        "   - Utilize the same subset of the ADNI dataset as used in the original study, ensuring each subject has imaging, genetic, and clinical data available.\n",
        "   - Follow the preprocessing steps described in the paper for each data modality.\n",
        "\n",
        "2. **Model Implementation:**\n",
        "   - Implement the unimodal models alone to measure their performance. Ensure that all other aspects of the model architecture remain constant to isolate the effect of the modality switch.\n",
        "\n",
        "3. **Training:**\n",
        "   - Train all models on the training subset of the ADNI dataset.\n",
        "\n",
        "4. **Evaluation:**\n",
        "   - Evaluate all models on a separate validation set from the ADNI dataset.\n",
        "   - Compare the performance metrics (accuracy, F1-score, precision, recall) of the models.\n",
        "\n",
        "5. **Analysis:**\n",
        "   - Analyze the results to determine if the unimodal models are sufficient for a good diagnostic accuracy.\n",
        "\n",
        "### Hypothesis 2: Integration of Multiple Data Modalities Leads to Higher Diagnostic Performance than Single Modality Approaches\n",
        "\n",
        "**Hypothesis Description:**\n",
        "The paper claims that the integration of multiple data modalities (imaging, genetic, clinical) using a multimodal deep learning approach significantly outperforms any single-modality model in diagnosing Alzheimer's disease stages.\n",
        "\n",
        "**Experiment to Test the Hypothesis:**\n",
        "1. **Data Preparation:**\n",
        "   - Use the same instances from the ADNI dataset, ensuring availability of imaging, genetic, and clinical data.\n",
        "   - Process each modality as per the guidelines detailed in the paper.\n",
        "\n",
        "2. **Model Implementation:**\n",
        "   - Develop three unimodal models for each data type: imaging (using convolutional neural networks), genetic (using fully connected networks), and clinical (using fully connected networks).\n",
        "\n",
        "3. **Training:**\n",
        "   - Train all models separately on their respective datasets.\n",
        "\n",
        "4. **Evaluation:**\n",
        "   - Evaluate each model on a common validation set from the ADNI dataset.\n",
        "   - Record and compare performance metrics for each model.\n",
        "\n",
        "5. **Analysis:**\n",
        "   - Determine whether the unimodal approach performs well in terms of accuracy, precision, recall, and F1-scores.\n",
        "\n",
        "These experiments are designed to validate the central claims of the paper regarding the efficacy of multimodal and attention-based methods in improving the diagnosis of Alzheimer's disease using the ADNI dataset when compared to the baseline unimodal models."
      ],
      "metadata": {
        "id": "KjhKUHFxXw6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology"
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment\n",
        "\n",
        "We conducted all implementations using Python version 3.11.6.\n",
        "\n",
        "A list of all packages/dependencies needed are:\n",
        "\n",
        "- TensorFlow\n",
        "- Keras\n",
        "- NumPy\n",
        "- Pandas\n",
        "- Pickle\n",
        "- Matplotlib\n",
        "- Seaborn\n",
        "- scikit-learn\n",
        "- os (standard)\n",
        "- random (standard)"
      ],
      "metadata": {
        "id": "h3N6z2-tQWJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "For our project, we will utilize the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, which provides a comprehensive collection of genetic, imaging, and clinical data. Our data processing will involve several steps to ensure the data is compatible with our model requirements:\n",
        "\n",
        "1. **Source of Data:** The data will be sourced from ADNI, specifically designed to assist in the study of Alzheimer’s disease progression. The dataset includes clinical assessments, imaging through MRI and PET scans, and genetic markers. For more information or to access the dataset, visit [ADNI's official site](https://adni.loni.usc.edu/).\n",
        "\n",
        "2. **Statistics:** We will process and analyze over 1500 individual patient records. Each record contains multimodal data types, including demographic information, genetic markers, and imaging data.\n",
        "\n",
        "3. **Data Processing:**\n",
        "   - Clinical and genetic data will be normalized and categorized as needed.\n",
        "   - Imaging data will be resized and standardized to ensure uniform input sizes for the neural network.\n",
        "   - Missing values will be handled by imputation or removal, depending on their frequency and impact on the dataset’s integrity."
      ],
      "metadata": {
        "id": "PXt1TpJpu4-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "This is a picture of the data and what data was collected from the paper.\n",
        "\n",
        "https://drive.google.com/file/d/1P85yipZm1QaKI1EjaAb5VOLSwE_6j-of/view?usp=sharing\n"
      ],
      "metadata": {
        "id": "8gPbJtVzRpfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine all diagnoses\n",
        "\n",
        "We will take diagnoses from images, clinical, and diagnosis sheet, and create one ground truth where all sources agree, and one majority vote where two sources agree."
      ],
      "metadata": {
        "id": "WA_xy1xx35X5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "clinical = pd.read_csv(\"data/ADSP_PHC_COGN.csv\") # .rename(columns={\"PHASE\":\"Phase\"})\n",
        "#this file is the metadata file that one can get from downloading MRI images from ADNI\n",
        "img = pd.read_csv(\"data/ADNI1_Annual_2_Yr_3T_4_14_2024.csv\")\n",
        "comb = pd.read_csv(\"data/DXSUM_PDXCONV_ADNIALL.csv\")[[\"RID\", \"PTID\" , \"PHASE\"]]\n",
        "\n",
        "def read_diagnose(file_path: str = 'data/DXSUM_PDXCONV_ADNIALL.csv', verbose=False):\n",
        "    # Read diagnostic summary\n",
        "    diagnostic_summary = pd.read_csv(file_path, index_col='PTID')\n",
        "    diagnostic_summary = diagnostic_summary.sort_values(by=[\"update_stamp\"], ascending=True)\n",
        "    # Create dictionary\n",
        "    diagnostic_dict: dict = {}\n",
        "    for key, data in diagnostic_summary.iterrows():\n",
        "        # Iterate for each row of the document\n",
        "        phase: str = data['PHASE']\n",
        "        diagnosis: float = -1.\n",
        "        if phase == \"ADNI1\":\n",
        "            diagnosis = data['DIAGNOSIS']\n",
        "        elif phase == \"ADNI2\" or phase == \"ADNIGO\":\n",
        "            dxchange = data['DIAGNOSIS']\n",
        "            if dxchange == 1 or dxchange == 7 or dxchange == 9:\n",
        "                diagnosis = 1.\n",
        "            if dxchange == 2 or dxchange == 4 or dxchange == 8:\n",
        "                diagnosis = 2.\n",
        "            if dxchange == 3 or dxchange == 5 or dxchange == 6:\n",
        "                diagnosis = 3.\n",
        "        elif phase == \"ADNI3\":\n",
        "            diagnosis = data['DIAGNOSIS']\n",
        "        else:\n",
        "            # print(f\"ERROR: Not recognized study phase {phase}\")\n",
        "            # exit(1)\n",
        "            pass\n",
        "        # Update dictionary\n",
        "        if not math.isnan(diagnosis):\n",
        "            diagnostic_dict[key] = diagnosis\n",
        "    if verbose:\n",
        "        print_diagnostic_dict_summary(diagnostic_dict)\n",
        "    return diagnostic_dict\n",
        "\n",
        "\n",
        "def print_diagnostic_dict_summary(diagnostic_dict: dict):\n",
        "    print(f\"Number of diagnosed patients: {len(diagnostic_dict.items())}\\n\")\n",
        "    n_NL = 0\n",
        "    n_MCI = 0\n",
        "    n_AD = 0\n",
        "    for (key, data) in diagnostic_dict.items():\n",
        "        if data == 1:\n",
        "            n_NL += 1\n",
        "        if data == 2:\n",
        "            n_MCI += 1\n",
        "        if data == 3:\n",
        "            n_AD += 1\n",
        "    print(f\"Number of NL patients: {n_NL}\\n\"\n",
        "          f\"Number of MCI patients: {n_MCI}\\n\"\n",
        "          f\"Number of AD patients: {n_AD}\\n\")\n",
        "\n",
        "d = read_diagnose()\n",
        "print_diagnostic_dict_summary(d)\n",
        "\n",
        "new = pd.DataFrame.from_dict(d, orient='index').reset_index()\n",
        "clinical[\"year\"] = clinical[\"EXAMDATE\"].str[:4]\n",
        "clinical[\"Subject\"] = clinical[\"SUBJECT_KEY\"].str.replace(\"ADNI_\", \"\").str.replace(\"s\", \"S\")\n",
        "c = comb.merge(clinical, on = [\"RID\", \"PHASE\"])\n",
        "c = c.drop(\"Subject\", axis =1)\n",
        "c = c.rename(columns = {\"PTID\":\"Subject\"})\n",
        "img[\"year\"] = img[\"Acq Date\"].str[5:].str.replace(\"/\", \"\")\n",
        "img = img.replace([\"CN\", \"MCI\", \"AD\"], [ 0, 1, 2])\n",
        "c[\"DX\"] = c[\"DX\"] -1\n",
        "new[0] = new[0].astype(int) -1\n",
        "new = new.rename(columns = {\"index\":\"Subject\", 0:\"GroupN\"})\n",
        "m = new.merge(c, on = \"Subject\", how = \"outer\").merge(img, on = \"Subject\", how = \"outer\")\n",
        "m[[\"GroupN\", \"DX\", \"Group\"]]\n",
        "m = m[[\"Subject\", \"GroupN\", \"Group\", \"DX\", \"PHASE\"]].drop_duplicates()\n",
        "m = m.dropna(subset = [\"GroupN\", \"Group\", \"DX\"], how=\"all\").drop_duplicates()\n",
        "m.loc[m[\"DX\"].isna() & m[\"Group\"].isna(), \"Group\"] = m.loc[m[\"DX\"].isna() & m[\"Group\"].isna(), \"GroupN\"]\n",
        "m.loc[m[\"DX\"].isna() & m[\"Group\"].isna(), \"DX\"] = m.loc[m[\"DX\"].isna() & m[\"Group\"].isna(), \"GroupN\"]\n",
        "m1 = m[m[\"GroupN\"] == m[\"Group\"]]\n",
        "m3 = m[m[\"GroupN\"] == m[\"DX\"]]\n",
        "m4 = m[m[\"Group\"] == m[\"DX\"]]\n",
        "m2 = m1[m1[\"Group\"] == m1[\"DX\"]]\n",
        "m1 = m1[[\"Subject\", \"GroupN\", \"Group\", \"DX\", \"PHASE\"]]\n",
        "m1.loc[m1[\"DX\"].isna(), \"DX\"] = m1.loc[m1[\"DX\"].isna(), \"Group\"]\n",
        "m3 = m3[[\"Subject\", \"GroupN\", \"Group\", \"DX\", \"PHASE\"]]\n",
        "m3.loc[m3[\"Group\"].isna(), \"Group\"] = m3.loc[m3[\"Group\"].isna(), \"GroupN\"]\n",
        "m4 = m4[[\"Subject\", \"GroupN\", \"Group\", \"DX\", \"PHASE\"]]\n",
        "m4[m4[\"GroupN\"] != m4[\"DX\"]]\n",
        "m5 = pd.concat([m1,m3,m4])\n",
        "i = m5[m5[\"Group\"] == m5[\"GroupN\"]]\n",
        "i = i[i[\"Group\"] == i[\"DX\"]]\n",
        "i = i.drop_duplicates()\n",
        "i[[\"Subject\", \"Group\", \"PHASE\"]].to_csv(\"ground_truth.csv\")\n",
        "m.update(m5[~m5.index.duplicated(keep='first')])\n",
        "indexes = m.index\n",
        "m[\"GROUP\"] = -1\n",
        "\n",
        "for i in indexes:\n",
        "    row = m.loc[i]\n",
        "    if (row[\"GroupN\"] == row[\"Group\"]):\n",
        "        val = row[\"GroupN\"]\n",
        "\n",
        "        m.loc[i, \"GROUP\"] = val\n",
        "    elif (row[\"GroupN\"] == row[\"DX\"]):\n",
        "        val = row[\"GroupN\"]\n",
        "        m.loc[i, \"GROUP\"] = val\n",
        "\n",
        "    elif (row[\"Group\"] == row[\"DX\"]):\n",
        "        val = row[\"Group\"]\n",
        "        m.loc[i, \"GROUP\"] = val\n",
        "m5 = m5[~m5.index.duplicated(keep='first')]\n",
        "m[m[\"GROUP\"] != -1]\n",
        "m[[\"Subject\", \"GroupN\", \"Group\", \"DX\", \"GROUP\", \"PHASE\"]].to_csv(\"diagnosis_full.csv\")"
      ],
      "metadata": {
        "id": "98h9N2nPzuWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b43b4e12-0037-4cf3-cb42-5d7d4f5b8cda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of diagnosed patients: 3033\n",
            "\n",
            "Number of NL patients: 1023\n",
            "Number of MCI patients: 958\n",
            "Number of AD patients: 879\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create clinical dataset\n",
        "\n",
        "We will now process raw CSV clinical data from ADNI into a format that can be standardized with other sources (imaging, genetic)"
      ],
      "metadata": {
        "id": "1oi0uRCL2eT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,MaxPooling1D, Flatten,BatchNormalization, GaussianNoise,Conv1D\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.utils import compute_class_weight\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Sequential, save_model, load_model\n",
        "\n",
        "#this was created in general/diagnosis_making notebook\n",
        "diag = pd.read_csv(\"ground_truth.csv\").drop(\"Unnamed: 0\", axis=1)\n",
        "# Below we are combining several clinical datasets.\n",
        "demo = pd.read_csv(\"data/PTDEMOG.csv\")\n",
        "neuro = pd.read_csv(\"data/NEUROEXM.csv\")\n",
        "\n",
        "\n",
        "neuro.columns\n",
        "\n",
        "clinical = pd.read_csv(\"data/ADSP_PHC_COGN.csv\") #.rename(columns={\"PHASE\":\"PHASE\"})\n",
        "clinical.head()\n",
        "diag[\"Subject\"].value_counts()\n",
        "\n",
        "comb = pd.read_csv(\"data/DXSUM_PDXCONV_ADNIALL.csv\")[[\"RID\", \"PTID\" , \"PHASE\"]]\n",
        "m = comb.merge(demo, on = [\"RID\", \"PHASE\"]).merge(neuro,on = [\"RID\", \"PHASE\"]).merge(clinical,on = [\"RID\", \"PHASE\"]).drop_duplicates()\n",
        "m.columns = [c[:-2] if str(c).endswith(('_x','_y')) else c for c in m.columns]\n",
        "m = m.loc[:,~m.columns.duplicated()]\n",
        "diag = diag.rename(columns = {\"Subject\": \"PTID\"})\n",
        "m = m.merge(diag, on = [\"PTID\", \"PHASE\"])\n",
        "m[\"PTID\"].value_counts()\n",
        "\n",
        "t = m\n",
        "t = t.drop([\"ID\",  \"SITEID\", \"VISCODE\", \"VISCODE2\", \"USERDATE\", \"USERDATE2\",\n",
        "            \"update_stamp\",  \"PTSOURCE\",\"DX\"], axis=1)\n",
        "\n",
        "t.columns\n",
        "t = t.fillna(-4)\n",
        "t = t.replace(\"-4\", -4)\n",
        "cols_to_delete = t.columns[(t == -4).sum()/len(t) > .70]\n",
        "t.drop(cols_to_delete, axis = 1, inplace = True)\n",
        "\n",
        "len(t.columns)\n",
        "print(t.columns)\n",
        "print(cols_to_delete)\n",
        "\n",
        "categorical = ['PTGENDER',\n",
        " 'PTHOME',\n",
        " 'PTMARRY',\n",
        " 'PTEDUCAT',\n",
        " 'PTPLANG',\n",
        " 'NXVISUAL',\n",
        " 'PTNOTRT',\n",
        " 'NXTREMOR',\n",
        " 'NXAUDITO',\n",
        " 'PTHAND']\n",
        "\n",
        "quant = ['PTDOBYY',\n",
        " 'PHC_MEM',\n",
        " 'PHC_EXF',\n",
        " 'PTRACCAT',\n",
        " 'AGE',\n",
        " 'PTADDX',\n",
        " 'PTETHCAT',\n",
        " 'PTCOGBEG',\n",
        " 'PHC_VSP',\n",
        " 'PHC_LAN']\n",
        "\n",
        "text = [\"PTWORK\", \"CMMED\", \"PTDOB\", \"VISDATE\"]\n",
        "cols_left = list(set(t.columns) - set(categorical) - set(text)  - set([\"label\", \"Group\",\"GROUP\", \"PHASE\", \"RID\", \"PTID\"]))\n",
        "t[cols_left]\n",
        "\n",
        "for col in cols_left:\n",
        "    if len(t[col].value_counts()) < 10:\n",
        "        print(col)\n",
        "        categorical.append(col)\n",
        "\n",
        "to_del = [\"PTRTYR\", \"EXAMDATE\", \"SUBJECT_KEY\"]\n",
        "t = t.drop(to_del, axis=1)\n",
        "\n",
        "quant = list(set(cols_left) - set(categorical) - set(text)  -set(to_del) - set([\"label\", \"Group\",\"GROUP\", \"PHASE\", \"RID\", \"PTID\"]))\n",
        "cols_left = list(set(cols_left) - set(categorical) - set(text) - set(quant) - set(to_del))\n",
        "\n",
        "#after reviewing the meaning of each column, these are the final ones\n",
        "l = ['RID', 'PTID', 'Group', 'PHASE', 'PTGENDER', 'PTDOBYY', 'PTHAND',\n",
        "       'PTMARRY', 'PTEDUCAT', 'PTNOTRT', 'PTHOME', 'PTTLANG',\n",
        "       'PTPLANG', 'PTCOGBEG', 'PTETHCAT', 'PTRACCAT', 'NXVISUAL',\n",
        "       'NXAUDITO', 'NXTREMOR', 'NXCONSCI', 'NXNERVE', 'NXMOTOR', 'NXFINGER',\n",
        "       'NXHEEL', 'NXSENSOR', 'NXTENDON', 'NXPLANTA', 'NXGAIT',\n",
        "       'NXABNORM',  'PHC_MEM', 'PHC_EXF', 'PHC_LAN', 'PHC_VSP']\n",
        "\n",
        "dfs = []\n",
        "\n",
        "for col in categorical:\n",
        "    dfs.append(pd.get_dummies(t[col], prefix = col, dtype=float))\n",
        "\n",
        "cat = pd.concat(dfs, axis=1)\n",
        "c = pd.concat([t[[\"PTID\", \"RID\", \"PHASE\", \"Group\"]].reset_index(), cat.reset_index(), t[quant].reset_index()], axis=1).drop(\"index\", axis=1) #tex\n",
        "#removing repeating subjects, taking the most recent diagnosis\n",
        "c = c.groupby('PTID',\n",
        "                  group_keys=False).apply(lambda x: x.loc[x[\"Group\"].astype(int).idxmax()]).drop(\"PTID\", axis = 1).reset_index(inplace=False)\n",
        "c.to_csv(\"clinical.csv\")\n",
        "\n",
        "\n",
        "#reading in the overlap test set\n",
        "# ts = pd.read_csv(\"overlap_test_set.csv\").rename(columns={\"subject\": \"PTID\"})\n",
        "# #removing ids from the overlap test set\n",
        "# c = c[~c[\"PTID\"].isin(list(ts[\"PTID\"].values))]\n",
        "\n",
        "cols = list(set(c.columns) - set([\"PTID\",\"RID\",\"subject\", \"ID\",\"GROUP\", \"Group\", \"label\", \"PHASE\", \"SITEID\", \"VISCODE\", \"VISCODE2\", \"USERDATE\", \"USERDATE2\", \"update_stamp\", \"DX_x\",\"DX_y\", \"Unnamed: 0\"]))\n",
        "X = c[cols].values\n",
        "y = c[\"Group\"].astype(int).values\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "# print(X_train[:1])\n",
        "\n",
        "import pickle\n",
        "\n",
        "print(\"X_train shape: \", X_train.shape, \"y_train shape: \", y_train.shape, \"X_test shape: \", X_test.shape, \"y_test shape: \", y_test.shape)\n",
        "\n",
        "with open('X_train_c.pkl', 'wb') as f:\n",
        "    pickle.dump(X_train, f)\n",
        "\n",
        "with open('X_test_c.pkl', 'wb') as f:\n",
        "    pickle.dump(X_test, f)\n",
        "\n",
        "with open('y_train_c.pkl', 'wb') as f:\n",
        "    pickle.dump(y_train, f)\n",
        "\n",
        "with open('y_test_c.pkl', 'wb') as f:\n",
        "    pickle.dump(y_test, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-mBr9d520jS",
        "outputId": "0273539d-c6c1-4a4c-e8ea-ce8688d7c5f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['RID', 'PTID', 'PHASE', 'VISDATE', 'PTGENDER', 'PTDOB', 'PTDOBYY',\n",
            "       'PTHAND', 'PTMARRY', 'PTEDUCAT', 'PTNOTRT', 'PTRTYR', 'PTHOME',\n",
            "       'PTTLANG', 'PTPLANG', 'PTCOGBEG', 'PTADDX', 'PTETHCAT', 'PTRACCAT',\n",
            "       'NXVISUAL', 'NXAUDITO', 'NXTREMOR', 'NXCONSCI', 'NXNERVE', 'NXMOTOR',\n",
            "       'NXFINGER', 'NXHEEL', 'NXSENSOR', 'NXTENDON', 'NXPLANTA', 'NXGAIT',\n",
            "       'NXOTHER', 'NXABNORM', 'SUBJECT_KEY', 'EXAMDATE', 'AGE', 'PHC_MEM',\n",
            "       'PHC_EXF', 'PHC_LAN', 'PHC_VSP', 'Group'],\n",
            "      dtype='object')\n",
            "Index(['PTWORKHS', 'PTWORK', 'PTADBEG', 'PTIDENT', 'PTENGSPK', 'PTNLANG',\n",
            "       'PTENGSPKAGE', 'PTCLANG', 'PTLANGSP', 'PTLANGWR', 'PTSPTIM',\n",
            "       'PTSPOTTIM', 'PTLANGPR1', 'PTLANGSP1', 'PTLANGRD1', 'PTLANGWR1',\n",
            "       'PTLANGUN1', 'PTLANGPR2', 'PTLANGSP2', 'PTLANGRD2', 'PTLANGWR2',\n",
            "       'PTLANGUN2', 'PTLANGPR3', 'PTLANGSP3', 'PTLANGRD3', 'PTLANGWR3',\n",
            "       'PTLANGUN3', 'PTLANGPR4', 'PTLANGSP4', 'PTLANGRD4', 'PTLANGWR4',\n",
            "       'PTLANGUN4', 'PTLANGPR5', 'PTLANGSP5', 'PTLANGRD5', 'PTLANGWR5',\n",
            "       'PTLANGUN5', 'PTLANGPR6', 'PTLANGSP6', 'PTLANGRD6', 'PTLANGWR6',\n",
            "       'PTLANGUN6', 'PTLANGTTL', 'PTETHCATH', 'PTBORN', 'PTBIRPL', 'PTIMMAGE',\n",
            "       'PTIMMWHY', 'PTBIRPR', 'PTBIRGR', 'DD_CRF_VERSION_LABEL',\n",
            "       'LANGUAGE_CODE', 'HAS_QC_ERROR'],\n",
            "      dtype='object')\n",
            "PTETHCAT\n",
            "NXGAIT\n",
            "PTTLANG\n",
            "NXTENDON\n",
            "NXFINGER\n",
            "NXHEEL\n",
            "NXMOTOR\n",
            "NXSENSOR\n",
            "NXPLANTA\n",
            "NXNERVE\n",
            "NXOTHER\n",
            "NXABNORM\n",
            "NXCONSCI\n",
            "PTRACCAT\n",
            "[[ 2.6400000e-01  1.0000000e+00  0.0000000e+00  1.0000000e+00\n",
            "   0.0000000e+00 -4.0000000e+00  0.0000000e+00  1.0000000e+00\n",
            "   0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00\n",
            "   1.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "   0.0000000e+00 -5.4100000e-01  0.0000000e+00  0.0000000e+00\n",
            "   0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00\n",
            "   0.0000000e+00  0.0000000e+00  0.0000000e+00  6.3599998e-01\n",
            "   1.0000000e+00  0.0000000e+00  1.0000000e+00  0.0000000e+00\n",
            "   0.0000000e+00  0.0000000e+00  1.9300000e+03  0.0000000e+00\n",
            "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "   0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00\n",
            "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "   0.0000000e+00  1.0000000e+00  1.0000000e+00  0.0000000e+00\n",
            "   7.8168400e+01  1.0000000e+00  0.0000000e+00  1.0000000e+00\n",
            "   0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00\n",
            "   1.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "   0.0000000e+00 -4.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "   0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00\n",
            "   0.0000000e+00  1.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "   0.0000000e+00  1.0000000e+00  0.0000000e+00  1.0000000e+00\n",
            "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "   1.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00\n",
            "   0.0000000e+00  0.0000000e+00  1.0000000e+00  7.7899998e-01\n",
            "   0.0000000e+00  0.0000000e+00  1.0000000e+00  1.0000000e+00\n",
            "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
            "   0.0000000e+00]]\n",
            "X_train shape:  (2161, 113) y_train shape:  (2161,) X_test shape:  (241, 113) y_test shape:  (241,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess MRI images\n",
        "\n",
        "The raw images provided by ADNI are already pre-processed with specific image correction steps. On top of these pre-processing steps, we create a full dataset that matches subjects from metadata with their MRI images that can be used for training/evaluation."
      ],
      "metadata": {
        "id": "jVN_82fh4SkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import skimage.transform as skTrans\n",
        "import nibabel as nib\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "\n",
        "\n",
        "def normalize_img(img_array):\n",
        "    maxes = np.quantile(img_array, 0.995, axis=(0, 1, 2))\n",
        "    return img_array / maxes\n",
        "\n",
        "def create_dataset(meta, meta_all, path_to_datadir):\n",
        "    files = os.listdir(path_to_datadir)\n",
        "    start = '_'\n",
        "    end = '.nii'\n",
        "\n",
        "    for file in files:\n",
        "        if file.endswith(end) == True:\n",
        "        # if file != '.DS_Store':\n",
        "            path = os.path.join(path_to_datadir, file)\n",
        "            print(path)\n",
        "            img_id = file.split(start)[-1].split(end)[0]\n",
        "            idx = meta[meta[\"Image Data ID\"] == img_id].index[0]\n",
        "            im = nib.load(path).get_fdata()\n",
        "            n_i, n_j, n_k = im.shape\n",
        "            center_i = (n_i - 1) // 2\n",
        "            center_j = (n_j - 1) // 2\n",
        "            center_k = (n_k - 1) // 2\n",
        "            im1 = skTrans.resize(im[center_i, :, :], (72, 72), order=1, preserve_range=True)\n",
        "            im2 = skTrans.resize(im[:, center_j, :], (72, 72), order=1, preserve_range=True)\n",
        "            im3 = skTrans.resize(im[:, :, center_k], (72, 72), order=1, preserve_range=True)\n",
        "            im = np.array([im1, im2, im3]).T\n",
        "            print(im.shape)\n",
        "            label = meta.at[idx, \"Group\"]\n",
        "            subject = meta.at[idx, \"Subject\"]\n",
        "            norm_im = normalize_img(im)\n",
        "\n",
        "            # Creating a temporary DataFrame and concatenating it\n",
        "            temp_df = pd.DataFrame([{\"img_array\": norm_im, \"label\": label, \"subject\": subject}])\n",
        "            meta_all = pd.concat([meta_all, temp_df], ignore_index=True)\n",
        "\n",
        "    # Save the final DataFrame\n",
        "    meta_all.to_pickle(\"mri_meta.pkl\")\n",
        "\n",
        "meta = pd.read_csv(\"meta.csv\")\n",
        "print(len(meta))\n",
        "meta = meta[[\"Image Data ID\", \"Group\", \"Subject\"]] #MCI = 0, CN =1, AD = 2\n",
        "meta[\"Group\"] = pd.factorize(meta[\"Group\"])[0]\n",
        "meta_all = pd.DataFrame(columns = [\"img_array\",\"label\",\"subject\"])\n",
        "create_dataset(meta, meta_all, \"imgs1/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbXp-wQr4UCP",
        "outputId": "1bf21006-8731-4c47-9adb-b6b48c25c17c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "306\n",
            "imgs1/ADNI_067_S_0290_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20080611160129027_S50875_I109187.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_136_S_0184_MR_MPR____N3__Scaled_Br_20070215174801158_S12474_I40191.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_130_S_0449_MR_MPR____N3__Scaled_Br_20071119102143821_S38619_I82686.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_136_S_0429_MR_MPR____N3__Scaled_Br_20070215221039819_S15882_I40392.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_023_S_0376_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20061201170258692_S13786_I31392.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_130_S_0886_MR_MPR____N3__Scaled_Br_20080220160240264_S41511_I91173.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_007_S_1206_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070713113644677_S25703_I59955.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_027_S_0307_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20061222185350351_S14336_I34168.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_002_S_1018_MR_MPR____N3__Scaled_Br_20070217032215330_S24312_I40828.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_032_S_0677_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20080220105435721_S19889_I90942.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_116_S_0487_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20080225094838615_S33717_I92420.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_127_S_0622_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070922104550551_S15986_I74496.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_116_S_1232_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20080223173022273_S45145_I91912.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_136_S_0195_MR_MPR____N3__Scaled_Br_20070215185520914_S12748_I40254.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_012_S_1009_MR_MPR____N3__Scaled_Br_20070711170219086_S22179_I59214.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_023_S_0916_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20061203143925761_S19523_I31540.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_031_S_1209_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20080220104120930_S25948_I90925.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_136_S_0086_MR_MPR____N3__Scaled_Br_20070215172221943_S14069_I40172.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_002_S_0413_MR_MPR____N3__Scaled_Br_20070216232854688_S14782_I40657.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_100_S_0190_MR_MPR____N3__Scaled_Br_20061213165243410_S15011_I33114.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_002_S_1268_MR_MPR____N3__Scaled_Br_20080604160614224_S50532_I108422.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_051_S_1123_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070628171146730_S25973_I58054.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_023_S_1126_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20080302124111638_S43059_I94863.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_051_S_1331_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20081014085902488_S53103_I120544.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_067_S_0607_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20061229190754863_S18034_I34866.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_023_S_0030_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20061204153813354_S8908_I31623.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_005_S_0324_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070828100626599_S31973_I70611.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_037_S_0303_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070802181222889_S13916_I64116.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_031_S_1066_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20080220103435137_S23507_I90916.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_005_S_0448_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070904174004430_S32787_I71302.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_002_S_0559_MR_MPR____N3__Scaled_Br_20070319121214158_S15922_I45126.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_005_S_0572_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20081013184405559_S54808_I120460.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_007_S_1222_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070713115714585_S26090_I59986.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_136_S_1227_MR_MPR____N3__Scaled_Br_20070810000731580_S26837_I66824.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_018_S_0335_MR_MPR____N3__Scaled_Br_20070101215011237_S15247_I35014.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_116_S_0382_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070508153859326_S15735_I53802.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_023_S_0031_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20071127114844832_S42753_I83389.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_027_S_1387_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070402182606620_S28276_I47757.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_031_S_0830_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20080225100608090_S39458_I92444.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_127_S_0260_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20061227143400735_S13858_I34371.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_136_S_0300_MR_MPR____N3__Scaled_Br_20070809231204041_S32709_I66768.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_023_S_1046_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070427201702633_S22606_I52107.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_130_S_0956_MR_MPR____N3__Scaled_Br_20070210223200198_S22511_I39200.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_018_S_0450_MR_MPR____N3__Scaled_Br_20070101221702956_S15825_I35029.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_053_S_0507_MR_MPR____N3__Scaled_Br_20070927074706802_S15256_I75459.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_130_S_0969_MR_MPR____N3__Scaled_Br_20070210223626600_S22655_I39203.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_018_S_0633_MR_MPR____N3__Scaled_Br_20070101223315779_S16900_I35039.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_127_S_0844_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20071127181826334_S22284_I83521.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_027_S_1081_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070327153912557_S26054_I47177.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_027_S_0404_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20061222193516770_S14494_I34195.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_027_S_1385_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070807092416718_S28972_I65374.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_023_S_0926_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20071113181008708_S41848_I81937.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_012_S_0689_MR_MPR____N3__Scaled_Br_20070923132626782_S36506_I74659.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_037_S_0501_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070911102616170_S35029_I72405.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_116_S_1249_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20080223173636549_S45487_I91921.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_016_S_1117_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20071229174509877_S25222_I86318.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_136_S_0579_MR_MPR____N3__Scaled_Br_20071127184324135_S34002_I83554.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_023_S_0963_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20071115093426943_S41289_I82205.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_002_S_1280_MR_MPR____N3__Scaled_Br_20070808154151968_S27230_I65874.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_027_S_0403_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070813143207736_S32184_I67380.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_027_S_0835_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070109231952406_S19496_I35653.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_027_S_1082_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070327155420107_S23987_I47186.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_126_S_0606_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070209190556936_S17816_I38887.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_016_S_1326_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20071229180439589_S29365_I86345.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_116_S_0649_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070731185839496_S18687_I63508.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_002_S_1261_MR_MPR____N3__Scaled_Br_20070807144436756_S27226_I65561.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_023_S_1190_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070427203443094_S25153_I52129.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_002_S_1070_MR_MPR____N3__Scaled_Br_20070217034203890_S24206_I40840.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_023_S_0625_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20061203133805576_S16766_I31500.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_023_S_1262_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20080421163348281_S47824_I103276.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_100_S_0015_MR_MPR____N3__Scaled_Br_20071118114815290_S41463_I82551.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_100_S_1286_MR_MPR____N3__Scaled_Br_20071119085444964_S28476_I82571.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_002_S_0729_MR_MPR____N3__Scaled_Br_20070217001301848_S17535_I40692.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_005_S_0814_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070923123736255_S19117_I74600.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_126_S_0605_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070209185049979_S15942_I38878.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_005_S_0553_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20061212135745131_S15928_I32755.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_136_S_0196_MR_MPR____N3__Scaled_Br_20070215192140032_S13831_I40269.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_136_S_0426_MR_MPR____N3__Scaled_Br_20070215213410384_S15017_I40378.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_023_S_0388_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20061203120855584_S14001_I31446.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_130_S_0505_MR_MPR____N3__Scaled_Br_20070210222802960_S20396_I39197.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_023_S_1247_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20070427204139431_S26861_I52138.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_018_S_0369_MR_MPR____N3__Scaled_Br_20070123180135564_S14343_I37190.nii\n",
            "(72, 72, 3)\n",
            "imgs1/ADNI_023_S_0604_MR_MPR__GradWarp__B1_Correction__N3__Scaled_Br_20081014175230216_S54484_I120798.nii\n",
            "(72, 72, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split MRI image dataset for training\n",
        "\n",
        "Since there are multiple MRI images per subject, we will find unique patient ID's then randomly select a set of training items and test items that we can then use directly in our model."
      ],
      "metadata": {
        "id": "tm2a3DW1GIP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "#reading in a dataframe that contains image arrays, patient IDs (\"subject\"), and diagnosis\n",
        "m2 = pd.read_pickle(\"mri_meta.pkl\")\n",
        "\n",
        "#cleaning patient IDs\n",
        "m2[\"subject\"] = m2[\"subject\"].str.replace(\"s\", \"S\").str.replace(\"\\n\", \"\")\n",
        "\n",
        "#reading in the overlap test set\n",
        "# ts = pd.read_csv(\"overlap_test_set.csv\")\n",
        "\n",
        "# #removing ids from the overlap test set\n",
        "# m2 = m2[~m2[\"subject\"].isin(list(ts[\"subject\"].values))]\n",
        "\n",
        "\n",
        "#there are 86 unique patients\n",
        "subjects = list(set(m2[\"subject\"].values))\n",
        "len(subjects)\n",
        "\n",
        "# We do not allow for any repeating patients in the testing set. We only allowed repetition during training, and no patient was included in both training and testing sets.\n",
        "#selecting 367 patient IDs\n",
        "picked_ids = random.sample(subjects, 9)\n",
        "\n",
        "\n",
        "#creating the test set out of the patient IDs\n",
        "test = pd.DataFrame(columns=[\"img_array\", \"subject\", \"label\"])\n",
        "\n",
        "for picked_id in picked_ids:\n",
        "    # Sample a single entry where the 'subject' column matches 'picked_id'\n",
        "    s = m2[m2[\"subject\"] == picked_id].sample(n=1)\n",
        "    # Concatenate the sampled DataFrame 's' with 'test'\n",
        "    test = pd.concat([test, s], ignore_index=True)\n",
        "\n",
        "\n",
        "indexes = list(set(m2.index) - set(test.index))\n",
        "\n",
        "\n",
        "#creating the training set using all the other data points\n",
        "train = m2[m2.index.isin(indexes)]\n",
        "\n",
        "\n",
        "train[[\"img_array\"]].to_pickle(\"img_train.pkl\")\n",
        "test[[\"img_array\"]].to_pickle(\"img_test.pkl\")\n",
        "\n",
        "print(train[[\"img_array\"]].shape)\n",
        "\n",
        "\n",
        "train[[\"label\"]].to_pickle(\"img_y_train.pkl\")\n",
        "test[[\"label\"]].to_pickle(\"img_y_test.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4MPNiah9H3Y",
        "outputId": "9972aa3f-92ca-439d-b7fe-e4b65fc94c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(74, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models + Training + Evaluation\n",
        "\n",
        "Here is a [link](https://github.com/rsinghlab/MADDi) to the original paper's repository, and below is a citation to the original paper:\n",
        "\n",
        "1.   Michal Golovanevsky, Carsten Eickhoff, Ritambhara Singh, Multimodal attention-based deep learning for Alzheimer’s disease diagnosis, Journal of the American Medical Informatics Association, Volume 29, Issue 12, December 2022, Pages 2014–2022, https://doi.org/10.1093/jamia/ocac168\n",
        "\n",
        "### Metrics Descriptions\n",
        "\n",
        "For evaluating performance, we use several key metrics to assess accuracy and reliability in classifying patients into one of three categories: control group (no Alzheimer's disease), Alzheimer's disease, and No Alzheimer's disease (at-risk but not diagnosed). Here's a concise description of each metric used:\n",
        "\n",
        "- **Test Accuracy:** This metric measures the overall effectiveness of the model in correctly predicting the categories across all test samples. It's the ratio of correct predictions to the total number of cases examined.\n",
        "- **Precision:** Precision is calculated for each category and indicates the accuracy of positive predictions. It is defined as the ratio of true positives to the sum of true and false positives. This metric helps us understand the likelihood that a patient diagnosed with Alzheimer's by the model actually has the disease.\n",
        "- **Recall:** Recall measures the model's ability to identify all relevant instances per category. For our case, it reflects the proportion of actual Alzheimer's patients correctly identified by the model out of all patients who actually have Alzheimer's.\n",
        "- **F1-Score:** The F1-score combines precision and recall into a single metric by taking their harmonic mean. A higher F1-score indicates a more robust model.\n",
        "\n",
        "###   Unimodal Clinical Model\n",
        "\n",
        "**Model Description:** We use the same model(s) architecture described in the paper for the individual modality for the clinical data using modified code from MADDi. We are unable to use the same exact data as the authors due to ADNI data/protocol updates which resulted in slight deviations in our approach and results. For clinical data, we use a three-layer fully connected network just as the paper describes.\n",
        "\n",
        "**Hyperparameters:** We use **100 epochs** with a **batch size of 32** and **learning rate of 0.0001** in our training. We use **3 dropout rates of 0.5, 0.3, and 0.2** in an attempt to reduce overfitting.\n",
        "\n",
        "**Computational Requirements:** We are using the standard Google Colab provided GPU. From the results, we observe an **~0.8s** average runtime for each epoch. We only use one trial with one random seed with 100 epochs."
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def reset_random_seeds(seed):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "def train():\n",
        "    # Load the data\n",
        "    X_train = pd.read_pickle(\"X_train_c.pkl\")\n",
        "    y_train = pd.read_pickle(\"y_train_c.pkl\")\n",
        "    X_test = pd.read_pickle(\"X_test_c.pkl\")\n",
        "    y_test = pd.read_pickle(\"y_test_c.pkl\")\n",
        "\n",
        "    # Adjust data types\n",
        "    X_train = X_train.astype('float32')\n",
        "    y_train = y_train.astype('int32')\n",
        "    X_test = X_test.astype('float32')\n",
        "    y_test = y_test.astype('int32')\n",
        "\n",
        "    # Build the model\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(113,)))  # Adjust the input shape to match the actual feature count\n",
        "    model.add(Dense(128, activation=\"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation=\"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(50, activation=\"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(3, activation=\"softmax\"))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
        "\n",
        "    # Model summary\n",
        "    model.summary()\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, epochs=100, validation_split=0.1, batch_size=32, verbose=1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    score = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
        "\n",
        "    # Predictions\n",
        "    test_predictions = model.predict(X_test)\n",
        "    test_label = to_categorical(y_test, 3)\n",
        "    true_label = np.argmax(test_label, axis=1)\n",
        "    predicted_label = np.argmax(test_predictions, axis=1)\n",
        "    cr = classification_report(true_label, predicted_label, output_dict=True)\n",
        "    print(\"Classification Report:\", cr)\n",
        "\n",
        "train()\n"
      ],
      "metadata": {
        "id": "gBdVZoTvsSFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d392ed8-9a9d-4eda-bccc-4ca7d239b38a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               14592     \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 128)               512       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 64)                256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 50)                3250      \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 50)                200       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 50)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3)                 153       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 27219 (106.32 KB)\n",
            "Trainable params: 26735 (104.43 KB)\n",
            "Non-trainable params: 484 (1.89 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "61/61 [==============================] - 5s 21ms/step - loss: 1.3170 - sparse_categorical_accuracy: 0.4208 - val_loss: 0.9833 - val_sparse_categorical_accuracy: 0.5300\n",
            "Epoch 2/100\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 1.1294 - sparse_categorical_accuracy: 0.5144 - val_loss: 0.8410 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 3/100\n",
            "61/61 [==============================] - 1s 14ms/step - loss: 1.0202 - sparse_categorical_accuracy: 0.5694 - val_loss: 0.8512 - val_sparse_categorical_accuracy: 0.6267\n",
            "Epoch 4/100\n",
            "61/61 [==============================] - 1s 16ms/step - loss: 1.0085 - sparse_categorical_accuracy: 0.6049 - val_loss: 0.8689 - val_sparse_categorical_accuracy: 0.6267\n",
            "Epoch 5/100\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 0.9698 - sparse_categorical_accuracy: 0.6065 - val_loss: 0.8705 - val_sparse_categorical_accuracy: 0.6359\n",
            "Epoch 6/100\n",
            "61/61 [==============================] - 0s 5ms/step - loss: 0.9691 - sparse_categorical_accuracy: 0.6091 - val_loss: 0.8702 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 7/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.9504 - sparse_categorical_accuracy: 0.6271 - val_loss: 0.8733 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 8/100\n",
            "61/61 [==============================] - 0s 5ms/step - loss: 0.9465 - sparse_categorical_accuracy: 0.6379 - val_loss: 0.8644 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 9/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.9277 - sparse_categorical_accuracy: 0.6312 - val_loss: 0.8599 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 10/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.9013 - sparse_categorical_accuracy: 0.6343 - val_loss: 0.8544 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 11/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.8999 - sparse_categorical_accuracy: 0.6502 - val_loss: 0.8549 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 12/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.8939 - sparse_categorical_accuracy: 0.6466 - val_loss: 0.8421 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 13/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.9047 - sparse_categorical_accuracy: 0.6456 - val_loss: 0.8370 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 14/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.9032 - sparse_categorical_accuracy: 0.6435 - val_loss: 0.8320 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 15/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.8795 - sparse_categorical_accuracy: 0.6579 - val_loss: 0.8288 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 16/100\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 0.8979 - sparse_categorical_accuracy: 0.6399 - val_loss: 0.8309 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 17/100\n",
            "61/61 [==============================] - 1s 19ms/step - loss: 0.8702 - sparse_categorical_accuracy: 0.6600 - val_loss: 0.8249 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 18/100\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 0.8497 - sparse_categorical_accuracy: 0.6559 - val_loss: 0.8226 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 19/100\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 0.8524 - sparse_categorical_accuracy: 0.6569 - val_loss: 0.8184 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 20/100\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 0.8316 - sparse_categorical_accuracy: 0.6523 - val_loss: 0.8195 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 21/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.8406 - sparse_categorical_accuracy: 0.6610 - val_loss: 0.8248 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 22/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.8491 - sparse_categorical_accuracy: 0.6626 - val_loss: 0.8156 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 23/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.8268 - sparse_categorical_accuracy: 0.6631 - val_loss: 0.8101 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 24/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.8573 - sparse_categorical_accuracy: 0.6615 - val_loss: 0.8085 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 25/100\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.8171 - sparse_categorical_accuracy: 0.6790 - val_loss: 0.8150 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 26/100\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 0.8316 - sparse_categorical_accuracy: 0.6672 - val_loss: 0.8163 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 27/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.8218 - sparse_categorical_accuracy: 0.6749 - val_loss: 0.8187 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 28/100\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 0.8131 - sparse_categorical_accuracy: 0.6749 - val_loss: 0.8146 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 29/100\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 0.8343 - sparse_categorical_accuracy: 0.6590 - val_loss: 0.8167 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 30/100\n",
            "61/61 [==============================] - 1s 16ms/step - loss: 0.7882 - sparse_categorical_accuracy: 0.6790 - val_loss: 0.8209 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 31/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.7985 - sparse_categorical_accuracy: 0.6770 - val_loss: 0.8195 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 32/100\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.8059 - sparse_categorical_accuracy: 0.6759 - val_loss: 0.8169 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 33/100\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 0.7928 - sparse_categorical_accuracy: 0.6759 - val_loss: 0.8169 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 34/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.8202 - sparse_categorical_accuracy: 0.6651 - val_loss: 0.8137 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 35/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.7933 - sparse_categorical_accuracy: 0.6764 - val_loss: 0.8118 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 36/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.8127 - sparse_categorical_accuracy: 0.6749 - val_loss: 0.8106 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 37/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.7839 - sparse_categorical_accuracy: 0.6836 - val_loss: 0.8079 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 38/100\n",
            "61/61 [==============================] - 1s 14ms/step - loss: 0.7951 - sparse_categorical_accuracy: 0.6749 - val_loss: 0.8083 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 39/100\n",
            "61/61 [==============================] - 1s 17ms/step - loss: 0.8035 - sparse_categorical_accuracy: 0.6698 - val_loss: 0.8083 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 40/100\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 0.7684 - sparse_categorical_accuracy: 0.6857 - val_loss: 0.8060 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 41/100\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.7907 - sparse_categorical_accuracy: 0.6708 - val_loss: 0.8042 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 42/100\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 0.7900 - sparse_categorical_accuracy: 0.6857 - val_loss: 0.8039 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 43/100\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 0.7919 - sparse_categorical_accuracy: 0.6816 - val_loss: 0.8051 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 44/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.7938 - sparse_categorical_accuracy: 0.6780 - val_loss: 0.8045 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 45/100\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 0.7895 - sparse_categorical_accuracy: 0.6878 - val_loss: 0.8047 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 46/100\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 0.7680 - sparse_categorical_accuracy: 0.6924 - val_loss: 0.8024 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 47/100\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 0.7795 - sparse_categorical_accuracy: 0.6852 - val_loss: 0.8034 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 48/100\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.7836 - sparse_categorical_accuracy: 0.6759 - val_loss: 0.8009 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 49/100\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.7779 - sparse_categorical_accuracy: 0.6903 - val_loss: 0.7997 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 50/100\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 0.7579 - sparse_categorical_accuracy: 0.6826 - val_loss: 0.8041 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 51/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.7781 - sparse_categorical_accuracy: 0.6806 - val_loss: 0.8060 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 52/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.7824 - sparse_categorical_accuracy: 0.6708 - val_loss: 0.8044 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 53/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.7562 - sparse_categorical_accuracy: 0.6847 - val_loss: 0.8026 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 54/100\n",
            "61/61 [==============================] - 1s 14ms/step - loss: 0.7671 - sparse_categorical_accuracy: 0.6934 - val_loss: 0.8002 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 55/100\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 0.7705 - sparse_categorical_accuracy: 0.6800 - val_loss: 0.8015 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 56/100\n",
            "61/61 [==============================] - 1s 14ms/step - loss: 0.7662 - sparse_categorical_accuracy: 0.6842 - val_loss: 0.8004 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 57/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.7690 - sparse_categorical_accuracy: 0.6852 - val_loss: 0.8009 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 58/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.7752 - sparse_categorical_accuracy: 0.6775 - val_loss: 0.8014 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 59/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.7700 - sparse_categorical_accuracy: 0.6914 - val_loss: 0.8002 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 60/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.7597 - sparse_categorical_accuracy: 0.6878 - val_loss: 0.7994 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 61/100\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.7774 - sparse_categorical_accuracy: 0.6965 - val_loss: 0.7986 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 62/100\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 0.7746 - sparse_categorical_accuracy: 0.6878 - val_loss: 0.7994 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 63/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.7693 - sparse_categorical_accuracy: 0.6878 - val_loss: 0.7988 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 64/100\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 0.7649 - sparse_categorical_accuracy: 0.6831 - val_loss: 0.7989 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 65/100\n",
            "61/61 [==============================] - 1s 14ms/step - loss: 0.7451 - sparse_categorical_accuracy: 0.6944 - val_loss: 0.7998 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 66/100\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 0.7652 - sparse_categorical_accuracy: 0.6929 - val_loss: 0.7984 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 67/100\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 0.7530 - sparse_categorical_accuracy: 0.6929 - val_loss: 0.7970 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 68/100\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 0.7636 - sparse_categorical_accuracy: 0.6842 - val_loss: 0.7980 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 69/100\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 0.7644 - sparse_categorical_accuracy: 0.6811 - val_loss: 0.7979 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 70/100\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.7492 - sparse_categorical_accuracy: 0.7032 - val_loss: 0.7980 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 71/100\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 0.7436 - sparse_categorical_accuracy: 0.6970 - val_loss: 0.7975 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 72/100\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 0.7511 - sparse_categorical_accuracy: 0.6960 - val_loss: 0.7980 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 73/100\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.7564 - sparse_categorical_accuracy: 0.6934 - val_loss: 0.7988 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 74/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.7417 - sparse_categorical_accuracy: 0.7001 - val_loss: 0.7981 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 75/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.7639 - sparse_categorical_accuracy: 0.6893 - val_loss: 0.7967 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 76/100\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 0.7644 - sparse_categorical_accuracy: 0.6836 - val_loss: 0.7962 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 77/100\n",
            "61/61 [==============================] - 0s 5ms/step - loss: 0.7629 - sparse_categorical_accuracy: 0.6872 - val_loss: 0.7956 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 78/100\n",
            "61/61 [==============================] - 0s 5ms/step - loss: 0.7451 - sparse_categorical_accuracy: 0.6944 - val_loss: 0.7964 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 79/100\n",
            "61/61 [==============================] - 0s 5ms/step - loss: 0.7295 - sparse_categorical_accuracy: 0.6991 - val_loss: 0.7984 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 80/100\n",
            "61/61 [==============================] - 0s 5ms/step - loss: 0.7573 - sparse_categorical_accuracy: 0.6862 - val_loss: 0.7985 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 81/100\n",
            "61/61 [==============================] - 0s 5ms/step - loss: 0.7499 - sparse_categorical_accuracy: 0.7099 - val_loss: 0.7961 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 82/100\n",
            "61/61 [==============================] - 0s 5ms/step - loss: 0.7374 - sparse_categorical_accuracy: 0.6965 - val_loss: 0.7963 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 83/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.7423 - sparse_categorical_accuracy: 0.7032 - val_loss: 0.7954 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 84/100\n",
            "61/61 [==============================] - 0s 5ms/step - loss: 0.7396 - sparse_categorical_accuracy: 0.7001 - val_loss: 0.7966 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 85/100\n",
            "61/61 [==============================] - 0s 5ms/step - loss: 0.7337 - sparse_categorical_accuracy: 0.7016 - val_loss: 0.7972 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 86/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.7340 - sparse_categorical_accuracy: 0.6991 - val_loss: 0.7976 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 87/100\n",
            "61/61 [==============================] - 0s 5ms/step - loss: 0.7429 - sparse_categorical_accuracy: 0.7022 - val_loss: 0.7970 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 88/100\n",
            "61/61 [==============================] - 0s 4ms/step - loss: 0.7391 - sparse_categorical_accuracy: 0.6960 - val_loss: 0.7971 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 89/100\n",
            "61/61 [==============================] - 0s 4ms/step - loss: 0.7283 - sparse_categorical_accuracy: 0.6888 - val_loss: 0.7977 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 90/100\n",
            "61/61 [==============================] - 0s 4ms/step - loss: 0.7493 - sparse_categorical_accuracy: 0.7006 - val_loss: 0.7990 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 91/100\n",
            "61/61 [==============================] - 0s 4ms/step - loss: 0.7355 - sparse_categorical_accuracy: 0.7011 - val_loss: 0.7980 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 92/100\n",
            "61/61 [==============================] - 0s 4ms/step - loss: 0.7470 - sparse_categorical_accuracy: 0.6929 - val_loss: 0.7981 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 93/100\n",
            "61/61 [==============================] - 0s 5ms/step - loss: 0.7345 - sparse_categorical_accuracy: 0.6919 - val_loss: 0.7992 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 94/100\n",
            "61/61 [==============================] - 0s 5ms/step - loss: 0.7295 - sparse_categorical_accuracy: 0.7011 - val_loss: 0.7977 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 95/100\n",
            "61/61 [==============================] - 0s 4ms/step - loss: 0.7681 - sparse_categorical_accuracy: 0.6914 - val_loss: 0.7981 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 96/100\n",
            "61/61 [==============================] - 0s 5ms/step - loss: 0.7248 - sparse_categorical_accuracy: 0.6924 - val_loss: 0.7976 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 97/100\n",
            "61/61 [==============================] - 0s 5ms/step - loss: 0.7177 - sparse_categorical_accuracy: 0.7088 - val_loss: 0.7979 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 98/100\n",
            "61/61 [==============================] - 0s 4ms/step - loss: 0.7384 - sparse_categorical_accuracy: 0.6980 - val_loss: 0.7969 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 99/100\n",
            "61/61 [==============================] - 0s 4ms/step - loss: 0.7418 - sparse_categorical_accuracy: 0.6965 - val_loss: 0.7967 - val_sparse_categorical_accuracy: 0.6682\n",
            "Epoch 100/100\n",
            "61/61 [==============================] - 0s 5ms/step - loss: 0.7276 - sparse_categorical_accuracy: 0.6991 - val_loss: 0.7953 - val_sparse_categorical_accuracy: 0.6682\n",
            "Test loss: 0.7615731954574585 / Test accuracy: 0.6929460763931274\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Classification Report: {'0': {'precision': 0.8888888888888888, 'recall': 0.6881720430107527, 'f1-score': 0.7757575757575758, 'support': 93}, '1': {'precision': 0.7027027027027027, 'recall': 0.611764705882353, 'f1-score': 0.6540880503144655, 'support': 85}, '2': {'precision': 0.5368421052631579, 'recall': 0.8095238095238095, 'f1-score': 0.6455696202531644, 'support': 63}, 'accuracy': 0.6929460580912863, 'macro avg': {'precision': 0.7094778989515831, 'recall': 0.7031535194723051, 'f1-score': 0.6918050821084019, 'support': 241}, 'weighted avg': {'precision': 0.7311927345559144, 'recall': 0.6929460580912863, 'f1-score': 0.6988125514445372, 'support': 241}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Discussion for Clinical Model\n",
        "\n",
        "With our limited data and training approach, we report an overall test accuracy of **~69.3%** across all 3 patient groups. Across these 3 groups, we obtain a weighted precision of **~73.1%**, a weighted recall of **~69.2%**, and an F1-score of **0.698**.\n",
        "\n",
        "Our overall accuracy is roughly ~10% below the authors reported accuracy of 80.59%. This is likely due to us using a smaller subset of the data compared to the authors implementation."
      ],
      "metadata": {
        "id": "kJd1miBBJqps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unimodal MRI Imaging Model\n",
        "\n",
        "The paper and its code use a large (and different) subset of patient imaging data compared to our approach. To ensure our code can run on Colab, we only used about 96 patients worth of data. Similar to the paper, we used a three-layer CNN.\n",
        "\n",
        "**Model Description:** We use the same model(s) architecture described in the paper for the individual modality for the clinical data using modified code from MADDi. We are unable to use the same exact data as the authors due to ADNI data/protocol updates which resulted in slight deviations in our approach and results. For clinical data, we use a three-layer fully connected network just as the paper describes.\n",
        "\n",
        "**Hyperparameters:** We use **50 epochs** with a **batch size of 32** and **learning rate of 0.001** in our training. We use **2 dropout rates of 0.5, and 0.3** in an attempt to reduce overfitting.\n",
        "\n",
        "**Computational Requirements:** We are using the standard Google Colab provided GPU. From the results, we observe an **~2.4s** average runtime for each epoch. We conduct 4 trial with 4 random seeds with 50 epochs each."
      ],
      "metadata": {
        "id": "ZIJIEp_UIF3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import pickle as pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.layers import Dense,Dropout,MaxPooling2D, Flatten, Conv2D\n",
        "\n",
        "\n",
        "def reset_random_seeds(seed):\n",
        "    os.environ['PYTHONHASHSEED']=str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "def train():\n",
        "\n",
        "    with open(\"img_train.pkl\", \"rb\") as fh:\n",
        "        data = pickle.load(fh)\n",
        "    X_train_ = pd.DataFrame(data)[\"img_array\"]\n",
        "\n",
        "    with open(\"img_test.pkl\", \"rb\") as fh:\n",
        "        data = pickle.load(fh)\n",
        "    X_test_ = pd.DataFrame(data)[\"img_array\"]\n",
        "\n",
        "    with open(\"img_y_train.pkl\", \"rb\") as fh:\n",
        "        data = pickle.load(fh)\n",
        "    y_train = np.array(pd.DataFrame(data)[\"label\"].values.astype(np.float32)).flatten()\n",
        "\n",
        "    with open(\"img_y_test.pkl\", \"rb\") as fh:\n",
        "        data = pickle.load(fh)\n",
        "    y_test = np.array(pd.DataFrame(data)[\"label\"].values.astype(np.float32)).flatten()\n",
        "\n",
        "\n",
        "    y_test[y_test == 2] = -1\n",
        "    y_test[y_test == 1] = 2\n",
        "    y_test[y_test == -1] = 1\n",
        "\n",
        "    y_train[y_train == 2] = -1\n",
        "    y_train[y_train == 1] = 2\n",
        "    y_train[y_train == -1] = 1\n",
        "\n",
        "\n",
        "    X_train = []\n",
        "    X_test = []\n",
        "\n",
        "    for i in range(len(X_train_)):\n",
        "        X_train.append(X_train_.values[i])\n",
        "\n",
        "    for i in range(len(X_test_)):\n",
        "        X_test.append(X_test_.values[i])\n",
        "\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    X_test = np.array(X_test)\n",
        "\n",
        "\n",
        "    acc = []\n",
        "    f1 = []\n",
        "    precision = []\n",
        "    recall = []\n",
        "    seeds = [10, 20, 30, 40]\n",
        "    for seed in seeds:\n",
        "        reset_random_seeds(seed)\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(100, (3, 3),  activation='relu', input_shape=(72, 72, 3)))\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Conv2D(50, (3, 3), activation='relu'))\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(3, activation = \"softmax\"))\n",
        "\n",
        "\n",
        "        model.compile(Adam(learning_rate = 0.001), \"sparse_categorical_crossentropy\", metrics = [\"sparse_categorical_accuracy\"])\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "\n",
        "        history = model.fit(X_train, y_train, epochs=50, batch_size=32,validation_split=0.1, verbose=1)\n",
        "\n",
        "        score = model.evaluate(X_test, y_test, verbose=0)\n",
        "        print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
        "        acc.append(score[1])\n",
        "\n",
        "        test_predictions = model.predict(X_test)\n",
        "        test_label = to_categorical(y_test,3)\n",
        "\n",
        "        true_label= np.argmax(test_label, axis =1)\n",
        "\n",
        "        predicted_label= np.argmax(test_predictions, axis =1)\n",
        "\n",
        "        cr = classification_report(true_label, predicted_label, output_dict=True)\n",
        "        precision.append(cr[\"macro avg\"][\"precision\"])\n",
        "        recall.append(cr[\"macro avg\"][\"recall\"])\n",
        "        f1.append(cr[\"macro avg\"][\"f1-score\"])\n",
        "\n",
        "    print(\"Avg accuracy: \" + str(np.array(acc).mean()))\n",
        "    print(\"Avg precision: \" + str(np.array(precision).mean()))\n",
        "    print(\"Avg recall: \" + str(np.array(recall).mean()))\n",
        "    print(\"Avg f1: \" + str(np.array(f1).mean()))\n",
        "    print(\"Std accuracy: \" + str(np.array(acc).std()))\n",
        "    print(\"Std precision: \" + str(np.array(precision).std()))\n",
        "    print(\"Std recall: \" + str(np.array(recall).std()))\n",
        "    print(\"Std f1: \" + str(np.array(f1).std()))\n",
        "    print(acc)\n",
        "    print(precision)\n",
        "    print(recall)\n",
        "    print(f1)\n",
        "\n",
        "train()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjlpVcB6_QfF",
        "outputId": "1ed5a7dc-9e83-4434-c983-73ee45b2e3b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 70, 70, 100)       2800      \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPoolin  (None, 35, 35, 100)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 35, 35, 100)       0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 33, 33, 50)        45050     \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPoolin  (None, 16, 16, 50)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 16, 16, 50)        0         \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 3)                 38403     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 86253 (336.93 KB)\n",
            "Trainable params: 86253 (336.93 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "3/3 [==============================] - 4s 637ms/step - loss: 1.1370 - sparse_categorical_accuracy: 0.3030 - val_loss: 1.0108 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 2/50\n",
            "3/3 [==============================] - 1s 362ms/step - loss: 1.1181 - sparse_categorical_accuracy: 0.3939 - val_loss: 1.0271 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 3/50\n",
            "3/3 [==============================] - 1s 322ms/step - loss: 1.0923 - sparse_categorical_accuracy: 0.3939 - val_loss: 1.0381 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 4/50\n",
            "3/3 [==============================] - 1s 323ms/step - loss: 1.1769 - sparse_categorical_accuracy: 0.3939 - val_loss: 1.0353 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 5/50\n",
            "3/3 [==============================] - 1s 328ms/step - loss: 1.0870 - sparse_categorical_accuracy: 0.4091 - val_loss: 1.0621 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 6/50\n",
            "3/3 [==============================] - 1s 323ms/step - loss: 1.0529 - sparse_categorical_accuracy: 0.3939 - val_loss: 1.0786 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 7/50\n",
            "3/3 [==============================] - 1s 343ms/step - loss: 1.0617 - sparse_categorical_accuracy: 0.3939 - val_loss: 1.0728 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 8/50\n",
            "3/3 [==============================] - 1s 310ms/step - loss: 1.0502 - sparse_categorical_accuracy: 0.4394 - val_loss: 1.0831 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 9/50\n",
            "3/3 [==============================] - 1s 398ms/step - loss: 1.0314 - sparse_categorical_accuracy: 0.5152 - val_loss: 1.0876 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 10/50\n",
            "3/3 [==============================] - 2s 555ms/step - loss: 1.0242 - sparse_categorical_accuracy: 0.5606 - val_loss: 1.0843 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 11/50\n",
            "3/3 [==============================] - 2s 511ms/step - loss: 1.0170 - sparse_categorical_accuracy: 0.5303 - val_loss: 1.0620 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 12/50\n",
            "3/3 [==============================] - 2s 543ms/step - loss: 1.0016 - sparse_categorical_accuracy: 0.4394 - val_loss: 1.0153 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 13/50\n",
            "3/3 [==============================] - 1s 310ms/step - loss: 1.0114 - sparse_categorical_accuracy: 0.4242 - val_loss: 0.9914 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 14/50\n",
            "3/3 [==============================] - 1s 300ms/step - loss: 0.9907 - sparse_categorical_accuracy: 0.5606 - val_loss: 0.9867 - val_sparse_categorical_accuracy: 0.7500\n",
            "Epoch 15/50\n",
            "3/3 [==============================] - 1s 332ms/step - loss: 0.9945 - sparse_categorical_accuracy: 0.5758 - val_loss: 0.9982 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 16/50\n",
            "3/3 [==============================] - 1s 357ms/step - loss: 0.9717 - sparse_categorical_accuracy: 0.5000 - val_loss: 1.0145 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 17/50\n",
            "3/3 [==============================] - 1s 320ms/step - loss: 0.9306 - sparse_categorical_accuracy: 0.5758 - val_loss: 1.0637 - val_sparse_categorical_accuracy: 0.2500\n",
            "Epoch 18/50\n",
            "3/3 [==============================] - 1s 314ms/step - loss: 0.8928 - sparse_categorical_accuracy: 0.6212 - val_loss: 1.0659 - val_sparse_categorical_accuracy: 0.2500\n",
            "Epoch 19/50\n",
            "3/3 [==============================] - 1s 332ms/step - loss: 0.8897 - sparse_categorical_accuracy: 0.5909 - val_loss: 1.0027 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 20/50\n",
            "3/3 [==============================] - 1s 314ms/step - loss: 0.8145 - sparse_categorical_accuracy: 0.6212 - val_loss: 1.0474 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 21/50\n",
            "3/3 [==============================] - 2s 618ms/step - loss: 0.8157 - sparse_categorical_accuracy: 0.6818 - val_loss: 1.0861 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 22/50\n",
            "3/3 [==============================] - 2s 657ms/step - loss: 0.7523 - sparse_categorical_accuracy: 0.7576 - val_loss: 1.0413 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 23/50\n",
            "3/3 [==============================] - 2s 665ms/step - loss: 0.7906 - sparse_categorical_accuracy: 0.5909 - val_loss: 0.9953 - val_sparse_categorical_accuracy: 0.7500\n",
            "Epoch 24/50\n",
            "3/3 [==============================] - 2s 671ms/step - loss: 0.7590 - sparse_categorical_accuracy: 0.6515 - val_loss: 1.1124 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 25/50\n",
            "3/3 [==============================] - 2s 563ms/step - loss: 0.7608 - sparse_categorical_accuracy: 0.6667 - val_loss: 1.1621 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 26/50\n",
            "3/3 [==============================] - 1s 333ms/step - loss: 0.7341 - sparse_categorical_accuracy: 0.7727 - val_loss: 1.1382 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 27/50\n",
            "3/3 [==============================] - 1s 347ms/step - loss: 0.6935 - sparse_categorical_accuracy: 0.7273 - val_loss: 1.1266 - val_sparse_categorical_accuracy: 0.2500\n",
            "Epoch 28/50\n",
            "3/3 [==============================] - 1s 320ms/step - loss: 0.7293 - sparse_categorical_accuracy: 0.6515 - val_loss: 1.0286 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 29/50\n",
            "3/3 [==============================] - 1s 367ms/step - loss: 0.6006 - sparse_categorical_accuracy: 0.7576 - val_loss: 0.9790 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 30/50\n",
            "3/3 [==============================] - 1s 326ms/step - loss: 0.5525 - sparse_categorical_accuracy: 0.8030 - val_loss: 0.9996 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 31/50\n",
            "3/3 [==============================] - 1s 376ms/step - loss: 0.4903 - sparse_categorical_accuracy: 0.8788 - val_loss: 1.1219 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 32/50\n",
            "3/3 [==============================] - 1s 355ms/step - loss: 0.5205 - sparse_categorical_accuracy: 0.8485 - val_loss: 1.2508 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 33/50\n",
            "3/3 [==============================] - 1s 446ms/step - loss: 0.4918 - sparse_categorical_accuracy: 0.8636 - val_loss: 1.1724 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 34/50\n",
            "3/3 [==============================] - 2s 511ms/step - loss: 0.4016 - sparse_categorical_accuracy: 0.8636 - val_loss: 1.1901 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 35/50\n",
            "3/3 [==============================] - 2s 596ms/step - loss: 0.3793 - sparse_categorical_accuracy: 0.8788 - val_loss: 1.3359 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 36/50\n",
            "3/3 [==============================] - 2s 447ms/step - loss: 0.3298 - sparse_categorical_accuracy: 0.9394 - val_loss: 1.2753 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 37/50\n",
            "3/3 [==============================] - 1s 352ms/step - loss: 0.3602 - sparse_categorical_accuracy: 0.8788 - val_loss: 1.3014 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 38/50\n",
            "3/3 [==============================] - 1s 366ms/step - loss: 0.3079 - sparse_categorical_accuracy: 0.8939 - val_loss: 1.4203 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 39/50\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.3452 - sparse_categorical_accuracy: 0.8636 - val_loss: 1.3887 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 40/50\n",
            "3/3 [==============================] - 1s 367ms/step - loss: 0.2968 - sparse_categorical_accuracy: 0.9242 - val_loss: 1.4497 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 41/50\n",
            "3/3 [==============================] - 1s 363ms/step - loss: 0.2643 - sparse_categorical_accuracy: 0.9394 - val_loss: 1.3774 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 42/50\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 0.2020 - sparse_categorical_accuracy: 0.9697 - val_loss: 1.3164 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 43/50\n",
            "3/3 [==============================] - 1s 322ms/step - loss: 0.2510 - sparse_categorical_accuracy: 0.8939 - val_loss: 1.4307 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 44/50\n",
            "3/3 [==============================] - 2s 620ms/step - loss: 0.2008 - sparse_categorical_accuracy: 0.9545 - val_loss: 1.9831 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 45/50\n",
            "3/3 [==============================] - 2s 543ms/step - loss: 0.3250 - sparse_categorical_accuracy: 0.8333 - val_loss: 1.7535 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 46/50\n",
            "3/3 [==============================] - 2s 577ms/step - loss: 0.1609 - sparse_categorical_accuracy: 0.9848 - val_loss: 1.5731 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 47/50\n",
            "3/3 [==============================] - 2s 328ms/step - loss: 0.2325 - sparse_categorical_accuracy: 0.8939 - val_loss: 1.4772 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 48/50\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.2187 - sparse_categorical_accuracy: 0.9394 - val_loss: 1.4943 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 49/50\n",
            "3/3 [==============================] - 1s 312ms/step - loss: 0.1216 - sparse_categorical_accuracy: 0.9697 - val_loss: 1.5484 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 50/50\n",
            "3/3 [==============================] - 1s 317ms/step - loss: 0.0982 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.6400 - val_sparse_categorical_accuracy: 0.5000\n",
            "Test loss: 0.15704317390918732 / Test accuracy: 1.0\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_8 (Conv2D)           (None, 70, 70, 100)       2800      \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPoolin  (None, 35, 35, 100)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 35, 35, 100)       0         \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 33, 33, 50)        45050     \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPoolin  (None, 16, 16, 50)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 16, 16, 50)        0         \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 3)                 38403     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 86253 (336.93 KB)\n",
            "Trainable params: 86253 (336.93 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "3/3 [==============================] - 4s 831ms/step - loss: 1.2124 - sparse_categorical_accuracy: 0.3788 - val_loss: 1.0254 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 2/50\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 1.1425 - sparse_categorical_accuracy: 0.3636 - val_loss: 1.0604 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 3/50\n",
            "3/3 [==============================] - 1s 340ms/step - loss: 1.1144 - sparse_categorical_accuracy: 0.2879 - val_loss: 1.1158 - val_sparse_categorical_accuracy: 0.1250\n",
            "Epoch 4/50\n",
            "3/3 [==============================] - 1s 343ms/step - loss: 1.1228 - sparse_categorical_accuracy: 0.2424 - val_loss: 1.1113 - val_sparse_categorical_accuracy: 0.1250\n",
            "Epoch 5/50\n",
            "3/3 [==============================] - 1s 360ms/step - loss: 1.1233 - sparse_categorical_accuracy: 0.3030 - val_loss: 1.1038 - val_sparse_categorical_accuracy: 0.2500\n",
            "Epoch 6/50\n",
            "3/3 [==============================] - 1s 350ms/step - loss: 1.1097 - sparse_categorical_accuracy: 0.3636 - val_loss: 1.0988 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 7/50\n",
            "3/3 [==============================] - 1s 331ms/step - loss: 1.0844 - sparse_categorical_accuracy: 0.4394 - val_loss: 1.0881 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 8/50\n",
            "3/3 [==============================] - 1s 307ms/step - loss: 1.0656 - sparse_categorical_accuracy: 0.4394 - val_loss: 1.0863 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 9/50\n",
            "3/3 [==============================] - 1s 354ms/step - loss: 1.0482 - sparse_categorical_accuracy: 0.5606 - val_loss: 1.0860 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 10/50\n",
            "3/3 [==============================] - 2s 522ms/step - loss: 1.0302 - sparse_categorical_accuracy: 0.5758 - val_loss: 1.0700 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 11/50\n",
            "3/3 [==============================] - 2s 514ms/step - loss: 1.0117 - sparse_categorical_accuracy: 0.5909 - val_loss: 1.0448 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 12/50\n",
            "3/3 [==============================] - 2s 548ms/step - loss: 0.9863 - sparse_categorical_accuracy: 0.5152 - val_loss: 1.0233 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 13/50\n",
            "3/3 [==============================] - 2s 398ms/step - loss: 0.9905 - sparse_categorical_accuracy: 0.4545 - val_loss: 0.9880 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 14/50\n",
            "3/3 [==============================] - 1s 351ms/step - loss: 1.0014 - sparse_categorical_accuracy: 0.4242 - val_loss: 0.9763 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 15/50\n",
            "3/3 [==============================] - 1s 359ms/step - loss: 0.9877 - sparse_categorical_accuracy: 0.5152 - val_loss: 1.0337 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 16/50\n",
            "3/3 [==============================] - 1s 313ms/step - loss: 0.9982 - sparse_categorical_accuracy: 0.4394 - val_loss: 1.1673 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 17/50\n",
            "3/3 [==============================] - 1s 358ms/step - loss: 1.0262 - sparse_categorical_accuracy: 0.4394 - val_loss: 1.1478 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 18/50\n",
            "3/3 [==============================] - 1s 367ms/step - loss: 0.9439 - sparse_categorical_accuracy: 0.6061 - val_loss: 1.0880 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 19/50\n",
            "3/3 [==============================] - 1s 361ms/step - loss: 0.9054 - sparse_categorical_accuracy: 0.6970 - val_loss: 1.0652 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 20/50\n",
            "3/3 [==============================] - 2s 528ms/step - loss: 0.8918 - sparse_categorical_accuracy: 0.7879 - val_loss: 1.0497 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 21/50\n",
            "3/3 [==============================] - 3s 851ms/step - loss: 0.8476 - sparse_categorical_accuracy: 0.6667 - val_loss: 1.0719 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 22/50\n",
            "3/3 [==============================] - 3s 603ms/step - loss: 0.9019 - sparse_categorical_accuracy: 0.4697 - val_loss: 1.1060 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 23/50\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 0.8759 - sparse_categorical_accuracy: 0.5152 - val_loss: 1.0902 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 24/50\n",
            "3/3 [==============================] - 2s 518ms/step - loss: 0.7892 - sparse_categorical_accuracy: 0.6818 - val_loss: 1.1242 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 25/50\n",
            "3/3 [==============================] - 1s 334ms/step - loss: 0.7835 - sparse_categorical_accuracy: 0.7273 - val_loss: 1.1188 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 26/50\n",
            "3/3 [==============================] - 1s 369ms/step - loss: 0.7664 - sparse_categorical_accuracy: 0.6515 - val_loss: 1.0813 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 27/50\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 0.6626 - sparse_categorical_accuracy: 0.7424 - val_loss: 1.1546 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 28/50\n",
            "3/3 [==============================] - 1s 331ms/step - loss: 0.7209 - sparse_categorical_accuracy: 0.6818 - val_loss: 1.1671 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 29/50\n",
            "3/3 [==============================] - 1s 427ms/step - loss: 0.6583 - sparse_categorical_accuracy: 0.7424 - val_loss: 1.0843 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 30/50\n",
            "3/3 [==============================] - 2s 562ms/step - loss: 0.5890 - sparse_categorical_accuracy: 0.7879 - val_loss: 1.1605 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 31/50\n",
            "3/3 [==============================] - 2s 608ms/step - loss: 0.6110 - sparse_categorical_accuracy: 0.8182 - val_loss: 1.2478 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 32/50\n",
            "3/3 [==============================] - 2s 521ms/step - loss: 0.5543 - sparse_categorical_accuracy: 0.8030 - val_loss: 1.2288 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 33/50\n",
            "3/3 [==============================] - 1s 306ms/step - loss: 0.4921 - sparse_categorical_accuracy: 0.8939 - val_loss: 1.3612 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 34/50\n",
            "3/3 [==============================] - 1s 333ms/step - loss: 0.5660 - sparse_categorical_accuracy: 0.7576 - val_loss: 1.4026 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 35/50\n",
            "3/3 [==============================] - 1s 347ms/step - loss: 0.4220 - sparse_categorical_accuracy: 0.9091 - val_loss: 1.6209 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 36/50\n",
            "3/3 [==============================] - 1s 322ms/step - loss: 0.3497 - sparse_categorical_accuracy: 0.9394 - val_loss: 1.8856 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 37/50\n",
            "3/3 [==============================] - 1s 358ms/step - loss: 0.4883 - sparse_categorical_accuracy: 0.8333 - val_loss: 1.8586 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 38/50\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.3450 - sparse_categorical_accuracy: 0.9242 - val_loss: 1.6567 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 39/50\n",
            "3/3 [==============================] - 1s 345ms/step - loss: 0.3263 - sparse_categorical_accuracy: 0.8636 - val_loss: 1.7085 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 40/50\n",
            "3/3 [==============================] - 1s 406ms/step - loss: 0.4275 - sparse_categorical_accuracy: 0.7879 - val_loss: 1.7258 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 41/50\n",
            "3/3 [==============================] - 2s 536ms/step - loss: 0.2303 - sparse_categorical_accuracy: 0.9545 - val_loss: 1.9171 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 42/50\n",
            "3/3 [==============================] - 2s 544ms/step - loss: 0.2846 - sparse_categorical_accuracy: 0.9394 - val_loss: 2.0907 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 43/50\n",
            "3/3 [==============================] - 2s 520ms/step - loss: 0.2906 - sparse_categorical_accuracy: 0.9545 - val_loss: 2.0547 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 44/50\n",
            "3/3 [==============================] - 2s 490ms/step - loss: 0.1969 - sparse_categorical_accuracy: 0.9394 - val_loss: 2.1813 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 45/50\n",
            "3/3 [==============================] - 2s 357ms/step - loss: 0.1933 - sparse_categorical_accuracy: 0.9242 - val_loss: 2.3881 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 46/50\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 0.1733 - sparse_categorical_accuracy: 0.9697 - val_loss: 2.5587 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 47/50\n",
            "3/3 [==============================] - 1s 332ms/step - loss: 0.1547 - sparse_categorical_accuracy: 0.9697 - val_loss: 2.6527 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 48/50\n",
            "3/3 [==============================] - 1s 337ms/step - loss: 0.1756 - sparse_categorical_accuracy: 0.9545 - val_loss: 2.6020 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 49/50\n",
            "3/3 [==============================] - 1s 306ms/step - loss: 0.1415 - sparse_categorical_accuracy: 0.9545 - val_loss: 2.6683 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 50/50\n",
            "3/3 [==============================] - 1s 372ms/step - loss: 0.1238 - sparse_categorical_accuracy: 0.9848 - val_loss: 3.0023 - val_sparse_categorical_accuracy: 0.5000\n",
            "Test loss: 0.24199768900871277 / Test accuracy: 0.8888888955116272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7c5331b0ae60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 178ms/step\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_10 (Conv2D)          (None, 70, 70, 100)       2800      \n",
            "                                                                 \n",
            " max_pooling2d_10 (MaxPooli  (None, 35, 35, 100)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 35, 35, 100)       0         \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 33, 33, 50)        45050     \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPooli  (None, 16, 16, 50)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 16, 16, 50)        0         \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 3)                 38403     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 86253 (336.93 KB)\n",
            "Trainable params: 86253 (336.93 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "3/3 [==============================] - 3s 712ms/step - loss: 1.2285 - sparse_categorical_accuracy: 0.3333 - val_loss: 1.0433 - val_sparse_categorical_accuracy: 0.2500\n",
            "Epoch 2/50\n",
            "3/3 [==============================] - 1s 355ms/step - loss: 1.0845 - sparse_categorical_accuracy: 0.4848 - val_loss: 1.0922 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 3/50\n",
            "3/3 [==============================] - 1s 342ms/step - loss: 1.0559 - sparse_categorical_accuracy: 0.5152 - val_loss: 1.0914 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 4/50\n",
            "3/3 [==============================] - 1s 380ms/step - loss: 1.0731 - sparse_categorical_accuracy: 0.3939 - val_loss: 1.0896 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 5/50\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 1.0797 - sparse_categorical_accuracy: 0.3939 - val_loss: 1.0867 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 6/50\n",
            "3/3 [==============================] - 1s 349ms/step - loss: 1.0372 - sparse_categorical_accuracy: 0.4697 - val_loss: 1.0855 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 7/50\n",
            "3/3 [==============================] - 1s 333ms/step - loss: 1.0534 - sparse_categorical_accuracy: 0.3788 - val_loss: 1.0687 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 8/50\n",
            "3/3 [==============================] - 1s 318ms/step - loss: 1.0688 - sparse_categorical_accuracy: 0.3636 - val_loss: 1.0587 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 9/50\n",
            "3/3 [==============================] - 1s 415ms/step - loss: 1.0313 - sparse_categorical_accuracy: 0.4848 - val_loss: 1.0803 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 10/50\n",
            "3/3 [==============================] - 2s 630ms/step - loss: 1.0290 - sparse_categorical_accuracy: 0.5303 - val_loss: 1.0902 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 11/50\n",
            "3/3 [==============================] - 2s 554ms/step - loss: 1.0251 - sparse_categorical_accuracy: 0.6667 - val_loss: 1.0907 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 12/50\n",
            "3/3 [==============================] - 2s 474ms/step - loss: 1.0212 - sparse_categorical_accuracy: 0.6061 - val_loss: 1.0879 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 13/50\n",
            "3/3 [==============================] - 1s 331ms/step - loss: 1.0205 - sparse_categorical_accuracy: 0.5606 - val_loss: 1.0842 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 14/50\n",
            "3/3 [==============================] - 1s 326ms/step - loss: 1.0087 - sparse_categorical_accuracy: 0.6212 - val_loss: 1.0972 - val_sparse_categorical_accuracy: 0.2500\n",
            "Epoch 15/50\n",
            "3/3 [==============================] - 1s 308ms/step - loss: 1.0197 - sparse_categorical_accuracy: 0.4242 - val_loss: 1.1204 - val_sparse_categorical_accuracy: 0.2500\n",
            "Epoch 16/50\n",
            "3/3 [==============================] - 1s 320ms/step - loss: 1.0510 - sparse_categorical_accuracy: 0.3182 - val_loss: 1.0868 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 17/50\n",
            "3/3 [==============================] - 1s 340ms/step - loss: 0.9576 - sparse_categorical_accuracy: 0.5758 - val_loss: 1.0291 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 18/50\n",
            "3/3 [==============================] - 1s 371ms/step - loss: 0.8990 - sparse_categorical_accuracy: 0.6667 - val_loss: 0.9867 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 19/50\n",
            "3/3 [==============================] - 1s 354ms/step - loss: 0.8918 - sparse_categorical_accuracy: 0.6818 - val_loss: 0.9989 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 20/50\n",
            "3/3 [==============================] - 2s 448ms/step - loss: 0.9060 - sparse_categorical_accuracy: 0.5758 - val_loss: 0.9774 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 21/50\n",
            "3/3 [==============================] - 2s 505ms/step - loss: 0.8228 - sparse_categorical_accuracy: 0.6970 - val_loss: 0.9811 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 22/50\n",
            "3/3 [==============================] - 2s 562ms/step - loss: 0.7632 - sparse_categorical_accuracy: 0.7273 - val_loss: 1.0380 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 23/50\n",
            "3/3 [==============================] - 2s 393ms/step - loss: 0.7638 - sparse_categorical_accuracy: 0.7121 - val_loss: 1.0978 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 24/50\n",
            "3/3 [==============================] - 1s 341ms/step - loss: 0.8596 - sparse_categorical_accuracy: 0.5758 - val_loss: 0.9992 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 25/50\n",
            "3/3 [==============================] - 1s 341ms/step - loss: 0.6994 - sparse_categorical_accuracy: 0.7273 - val_loss: 0.9527 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 26/50\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 0.6986 - sparse_categorical_accuracy: 0.6818 - val_loss: 1.0344 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 27/50\n",
            "3/3 [==============================] - 1s 330ms/step - loss: 0.6963 - sparse_categorical_accuracy: 0.7727 - val_loss: 1.0280 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 28/50\n",
            "3/3 [==============================] - 1s 354ms/step - loss: 0.6397 - sparse_categorical_accuracy: 0.8030 - val_loss: 0.9504 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 29/50\n",
            "3/3 [==============================] - 1s 341ms/step - loss: 0.5481 - sparse_categorical_accuracy: 0.8182 - val_loss: 0.9802 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 30/50\n",
            "3/3 [==============================] - 1s 327ms/step - loss: 0.6075 - sparse_categorical_accuracy: 0.7879 - val_loss: 1.0217 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 31/50\n",
            "3/3 [==============================] - 2s 513ms/step - loss: 0.5846 - sparse_categorical_accuracy: 0.8485 - val_loss: 1.1263 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 32/50\n",
            "3/3 [==============================] - 2s 603ms/step - loss: 0.6195 - sparse_categorical_accuracy: 0.7576 - val_loss: 1.1455 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 33/50\n",
            "3/3 [==============================] - 2s 548ms/step - loss: 0.5667 - sparse_categorical_accuracy: 0.8182 - val_loss: 1.0732 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 34/50\n",
            "3/3 [==============================] - 2s 360ms/step - loss: 0.5619 - sparse_categorical_accuracy: 0.8485 - val_loss: 1.0084 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 35/50\n",
            "3/3 [==============================] - 1s 332ms/step - loss: 0.6015 - sparse_categorical_accuracy: 0.7424 - val_loss: 1.0094 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 36/50\n",
            "3/3 [==============================] - 1s 357ms/step - loss: 0.4814 - sparse_categorical_accuracy: 0.8182 - val_loss: 1.0895 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 37/50\n",
            "3/3 [==============================] - 1s 327ms/step - loss: 0.4157 - sparse_categorical_accuracy: 0.8788 - val_loss: 1.2472 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 38/50\n",
            "3/3 [==============================] - 1s 307ms/step - loss: 0.3972 - sparse_categorical_accuracy: 0.8788 - val_loss: 1.4511 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 39/50\n",
            "3/3 [==============================] - 1s 334ms/step - loss: 0.4502 - sparse_categorical_accuracy: 0.8788 - val_loss: 1.4680 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 40/50\n",
            "3/3 [==============================] - 1s 390ms/step - loss: 0.3020 - sparse_categorical_accuracy: 0.9242 - val_loss: 1.4263 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 41/50\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.2961 - sparse_categorical_accuracy: 0.9394 - val_loss: 1.4588 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 42/50\n",
            "3/3 [==============================] - 2s 597ms/step - loss: 0.3218 - sparse_categorical_accuracy: 0.8788 - val_loss: 1.5333 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 43/50\n",
            "3/3 [==============================] - 2s 606ms/step - loss: 0.2805 - sparse_categorical_accuracy: 0.8939 - val_loss: 1.6691 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 44/50\n",
            "3/3 [==============================] - 2s 558ms/step - loss: 0.2345 - sparse_categorical_accuracy: 0.9394 - val_loss: 1.7332 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 45/50\n",
            "3/3 [==============================] - 2s 365ms/step - loss: 0.2300 - sparse_categorical_accuracy: 0.9242 - val_loss: 1.7720 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 46/50\n",
            "3/3 [==============================] - 1s 382ms/step - loss: 0.1813 - sparse_categorical_accuracy: 0.9697 - val_loss: 1.8071 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 47/50\n",
            "3/3 [==============================] - 1s 314ms/step - loss: 0.1473 - sparse_categorical_accuracy: 0.9394 - val_loss: 1.8932 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 48/50\n",
            "3/3 [==============================] - 1s 334ms/step - loss: 0.1892 - sparse_categorical_accuracy: 0.9394 - val_loss: 1.9301 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 49/50\n",
            "3/3 [==============================] - 1s 343ms/step - loss: 0.1384 - sparse_categorical_accuracy: 0.9697 - val_loss: 1.9993 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 50/50\n",
            "3/3 [==============================] - 1s 326ms/step - loss: 0.1206 - sparse_categorical_accuracy: 0.9697 - val_loss: 2.0309 - val_sparse_categorical_accuracy: 0.5000\n",
            "Test loss: 0.2152988761663437 / Test accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7c53315ee0e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 120ms/step\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_12 (Conv2D)          (None, 70, 70, 100)       2800      \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPooli  (None, 35, 35, 100)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 35, 35, 100)       0         \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 33, 33, 50)        45050     \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPooli  (None, 16, 16, 50)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 16, 16, 50)        0         \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 3)                 38403     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 86253 (336.93 KB)\n",
            "Trainable params: 86253 (336.93 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "3/3 [==============================] - 2s 417ms/step - loss: 1.1312 - sparse_categorical_accuracy: 0.4091 - val_loss: 0.9761 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 2/50\n",
            "3/3 [==============================] - 2s 697ms/step - loss: 1.3979 - sparse_categorical_accuracy: 0.3939 - val_loss: 1.0079 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 3/50\n",
            "3/3 [==============================] - 2s 666ms/step - loss: 1.0949 - sparse_categorical_accuracy: 0.3939 - val_loss: 1.0635 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 4/50\n",
            "3/3 [==============================] - 3s 693ms/step - loss: 1.0804 - sparse_categorical_accuracy: 0.4091 - val_loss: 1.0881 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 5/50\n",
            "3/3 [==============================] - 2s 625ms/step - loss: 1.0830 - sparse_categorical_accuracy: 0.3636 - val_loss: 1.0845 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 6/50\n",
            "3/3 [==============================] - 2s 493ms/step - loss: 1.0788 - sparse_categorical_accuracy: 0.3636 - val_loss: 1.0803 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 7/50\n",
            "3/3 [==============================] - 1s 378ms/step - loss: 1.0660 - sparse_categorical_accuracy: 0.3939 - val_loss: 1.0742 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 8/50\n",
            "3/3 [==============================] - 1s 333ms/step - loss: 1.0516 - sparse_categorical_accuracy: 0.4091 - val_loss: 1.0747 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 9/50\n",
            "3/3 [==============================] - 1s 351ms/step - loss: 1.0512 - sparse_categorical_accuracy: 0.4242 - val_loss: 1.0723 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 10/50\n",
            "3/3 [==============================] - 1s 374ms/step - loss: 1.0390 - sparse_categorical_accuracy: 0.4091 - val_loss: 1.0621 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 11/50\n",
            "3/3 [==============================] - 1s 364ms/step - loss: 1.0483 - sparse_categorical_accuracy: 0.4091 - val_loss: 1.0664 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 12/50\n",
            "3/3 [==============================] - 1s 344ms/step - loss: 1.0407 - sparse_categorical_accuracy: 0.3939 - val_loss: 1.0516 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 13/50\n",
            "3/3 [==============================] - 1s 345ms/step - loss: 1.0386 - sparse_categorical_accuracy: 0.3939 - val_loss: 1.0283 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 14/50\n",
            "3/3 [==============================] - 2s 514ms/step - loss: 1.0344 - sparse_categorical_accuracy: 0.3939 - val_loss: 0.9981 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 15/50\n",
            "3/3 [==============================] - 2s 540ms/step - loss: 1.0476 - sparse_categorical_accuracy: 0.4242 - val_loss: 0.9765 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 16/50\n",
            "3/3 [==============================] - 2s 630ms/step - loss: 1.0307 - sparse_categorical_accuracy: 0.4697 - val_loss: 0.9662 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 17/50\n",
            "3/3 [==============================] - 1s 317ms/step - loss: 1.0260 - sparse_categorical_accuracy: 0.4848 - val_loss: 0.9825 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 18/50\n",
            "3/3 [==============================] - 1s 325ms/step - loss: 0.9923 - sparse_categorical_accuracy: 0.5303 - val_loss: 1.0404 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 19/50\n",
            "3/3 [==============================] - 1s 358ms/step - loss: 1.0031 - sparse_categorical_accuracy: 0.5455 - val_loss: 1.0846 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 20/50\n",
            "3/3 [==============================] - 1s 344ms/step - loss: 0.9982 - sparse_categorical_accuracy: 0.5606 - val_loss: 1.0867 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 21/50\n",
            "3/3 [==============================] - 1s 307ms/step - loss: 0.9809 - sparse_categorical_accuracy: 0.6212 - val_loss: 1.0414 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 22/50\n",
            "3/3 [==============================] - 1s 360ms/step - loss: 0.9385 - sparse_categorical_accuracy: 0.6515 - val_loss: 0.9790 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 23/50\n",
            "3/3 [==============================] - 1s 319ms/step - loss: 0.9647 - sparse_categorical_accuracy: 0.4848 - val_loss: 0.9455 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 24/50\n",
            "3/3 [==============================] - 1s 313ms/step - loss: 0.9787 - sparse_categorical_accuracy: 0.4545 - val_loss: 0.9404 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 25/50\n",
            "3/3 [==============================] - 2s 535ms/step - loss: 0.9355 - sparse_categorical_accuracy: 0.5000 - val_loss: 0.9615 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 26/50\n",
            "3/3 [==============================] - 2s 563ms/step - loss: 0.8104 - sparse_categorical_accuracy: 0.6515 - val_loss: 1.0242 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 27/50\n",
            "3/3 [==============================] - 2s 606ms/step - loss: 0.8088 - sparse_categorical_accuracy: 0.7273 - val_loss: 1.0303 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 28/50\n",
            "3/3 [==============================] - 1s 324ms/step - loss: 0.8110 - sparse_categorical_accuracy: 0.6061 - val_loss: 0.9815 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 29/50\n",
            "3/3 [==============================] - 1s 338ms/step - loss: 0.7676 - sparse_categorical_accuracy: 0.6515 - val_loss: 1.0040 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 30/50\n",
            "3/3 [==============================] - 1s 328ms/step - loss: 0.7778 - sparse_categorical_accuracy: 0.6364 - val_loss: 1.1735 - val_sparse_categorical_accuracy: 0.6250\n",
            "Epoch 31/50\n",
            "3/3 [==============================] - 1s 312ms/step - loss: 0.8855 - sparse_categorical_accuracy: 0.5758 - val_loss: 1.0976 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 32/50\n",
            "3/3 [==============================] - 1s 379ms/step - loss: 0.6613 - sparse_categorical_accuracy: 0.7727 - val_loss: 1.0186 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 33/50\n",
            "3/3 [==============================] - 1s 375ms/step - loss: 0.7589 - sparse_categorical_accuracy: 0.6061 - val_loss: 0.9840 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 34/50\n",
            "3/3 [==============================] - 1s 365ms/step - loss: 0.6858 - sparse_categorical_accuracy: 0.7273 - val_loss: 0.9541 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 35/50\n",
            "3/3 [==============================] - 1s 335ms/step - loss: 0.5977 - sparse_categorical_accuracy: 0.8333 - val_loss: 1.0283 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 36/50\n",
            "3/3 [==============================] - 2s 571ms/step - loss: 0.6146 - sparse_categorical_accuracy: 0.7424 - val_loss: 1.0739 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 37/50\n",
            "3/3 [==============================] - 4s 999ms/step - loss: 0.5409 - sparse_categorical_accuracy: 0.8182 - val_loss: 1.1703 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 38/50\n",
            "3/3 [==============================] - 5s 2s/step - loss: 0.5740 - sparse_categorical_accuracy: 0.7727 - val_loss: 1.2506 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 39/50\n",
            "3/3 [==============================] - 3s 522ms/step - loss: 0.5531 - sparse_categorical_accuracy: 0.7727 - val_loss: 1.2423 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 40/50\n",
            "3/3 [==============================] - 1s 360ms/step - loss: 0.4367 - sparse_categorical_accuracy: 0.8788 - val_loss: 1.4221 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 41/50\n",
            "3/3 [==============================] - 1s 306ms/step - loss: 0.4181 - sparse_categorical_accuracy: 0.8333 - val_loss: 1.6026 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 42/50\n",
            "3/3 [==============================] - 1s 329ms/step - loss: 0.3268 - sparse_categorical_accuracy: 0.9242 - val_loss: 1.8034 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 43/50\n",
            "3/3 [==============================] - 1s 351ms/step - loss: 0.3239 - sparse_categorical_accuracy: 0.8788 - val_loss: 1.9975 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 44/50\n",
            "3/3 [==============================] - 1s 395ms/step - loss: 0.3217 - sparse_categorical_accuracy: 0.8788 - val_loss: 2.1233 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 45/50\n",
            "3/3 [==============================] - 1s 359ms/step - loss: 0.2513 - sparse_categorical_accuracy: 0.9242 - val_loss: 2.3137 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 46/50\n",
            "3/3 [==============================] - 2s 474ms/step - loss: 0.2797 - sparse_categorical_accuracy: 0.9091 - val_loss: 2.4227 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 47/50\n",
            "3/3 [==============================] - 2s 503ms/step - loss: 0.1598 - sparse_categorical_accuracy: 1.0000 - val_loss: 2.4879 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 48/50\n",
            "3/3 [==============================] - 2s 505ms/step - loss: 0.1756 - sparse_categorical_accuracy: 0.9394 - val_loss: 2.7104 - val_sparse_categorical_accuracy: 0.3750\n",
            "Epoch 49/50\n",
            "3/3 [==============================] - 2s 534ms/step - loss: 0.2230 - sparse_categorical_accuracy: 0.9394 - val_loss: 2.5576 - val_sparse_categorical_accuracy: 0.5000\n",
            "Epoch 50/50\n",
            "3/3 [==============================] - 1s 326ms/step - loss: 0.1124 - sparse_categorical_accuracy: 0.9848 - val_loss: 2.6298 - val_sparse_categorical_accuracy: 0.5000\n",
            "Test loss: 0.20845256745815277 / Test accuracy: 0.8888888955116272\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "Avg accuracy: 0.9444444477558136\n",
            "Avg precision: 0.9666666666666666\n",
            "Avg recall: 0.9583333333333333\n",
            "Avg f1: 0.9576719576719577\n",
            "Std accuracy: 0.0555555522441864\n",
            "Std precision: 0.03333333333333338\n",
            "Std recall: 0.041666666666666685\n",
            "Std f1: 0.042328042328042326\n",
            "[1.0, 0.8888888955116272, 1.0, 0.8888888955116272]\n",
            "[1.0, 0.9333333333333332, 1.0, 0.9333333333333332]\n",
            "[1.0, 0.9166666666666666, 1.0, 0.9166666666666666]\n",
            "[1.0, 0.9153439153439153, 1.0, 0.9153439153439153]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Discussion for Imaging Model\n",
        "\n",
        "With our limited data and training approach, we report an overall test accuracy of **~94.4%** across all 3 patient groups. Across these 3 groups, we obtain an average precision of **~96.6%**, an average recall of **~95.8%**, and an F1-score of **0.95**.\n",
        "\n",
        "Contrary to what we expected, our overall accuracy actually ended up being roughly 2% higher than the authors highlighted. We believe this to also be due to the fact that we used a different and smaller subset of data. This likely led to our model overfitting somewhat which may contribute to the higher accuracy."
      ],
      "metadata": {
        "id": "iQzDwWc7LwI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "We used the training routine from the MADDi repository to access each individual modality and the collective model. The report with key metrics is also saved."
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plans\n",
        "\n",
        "For testing hypothesis 1, we will implement the models as described in the paper. We can then train and evaluate them against each other.\n",
        "\n",
        "For testing hypothesis 2, we were unable to get a multimodal model working with adequate accuracy. The issue is outlined further in the Discussion section. Since this is crucial to evaluation of hypothesis 2, we will offer the paper's results on this matter and discuss how these results would translate to our own implementations."
      ],
      "metadata": {
        "id": "AfS8XrVRoDCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### These are results for the clinical and imaging model respectively and they are explained above and discussed below:"
      ],
      "metadata": {
        "id": "gASZc4fWe3kE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://drive.google.com/file/d/1--oRuwYDjus_LzwB42XJ02O1r8dw4nig/view?usp=sharing\n",
        "\n",
        "https://drive.google.com/file/d/18cF8Nd-71RI6nWlgQmKs8-WlmnPwCrof/view?usp=drive_link"
      ],
      "metadata": {
        "id": "uLpoMp9Le90G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ablation studies\n",
        "\n",
        "In this study, ablation studies were conducted to evaluate the importance of attention mechanisms in the proposed model architecture for diagnosing Alzheimer's disease. The attention mechanisms in question include self-attention and cross-modal attention. These attention modules are responsible for capturing intermodal interactions and highlighting relevant features for decision-making in the model.\n",
        "\n",
        "By systematically disabling or including different attention mechanisms in the model, the paper was able to assess how each component contributes to the model's overall performance in AD diagnosis. The evaluation metrics used in the ablation studies include accuracy, precision, recall, and F1-score, which provide insights into the model's classification performance across different conditions.\n",
        "\n"
      ],
      "metadata": {
        "id": "gJN16WqzhsdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Make assessment that the paper is reproducible or not.\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "  * What will you do in next phase.\n",
        "\n"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">The paper presents a detailed methodology for diagnosing Alzheimer's Disease using a multimodal deep learning approach, specifically introducing the MADDi framework. Overall, the methodology provides clear steps for data processing, model creation, and evaluation, which enhances the reproducibility of the research. The paper specifies the source of the data used in the research, which is the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Additionally, it provides guidelines for accessing the dataset, ensuring that other researchers can obtain the same data for replication purposes. The paper includes code snippets for various aspects of the research, such as data preprocessing, model creation, and evaluation. These code snippets serve as a reference for implementing the methodology and conducting the experiments, making it easier for other researchers to replicate the analysis.\n",
        "\n",
        ">The process of reproducing the paper had both straightforward aspects and challenges. The clarity of the methodology and availability of code snippets facilitated the implementation of data processing steps and model creation. However, adapting the code to specific dataset characteristics and ensuring consistency with the paper's experimental setup posed challenges. Moreover, interpreting and comparing results required careful attention to detail and thorough understanding of the proposed methodology. Specifically, reproducing the multimodal modal's methodology proved to be quite difficult. Due to some data constraints, we were unable to get a multimodal model working with adequate accuracy (https://github.com/rsinghlab/MADDi/issues/17).\n",
        "\n",
        ">To enhance reproducibility, future authors could provide more detailed documentation of dataset characteristics, preprocessing steps, and model hyperparameters. Standardizing the evaluation metrics and providing comprehensive explanations of experimental choices would also aid in replicating the research."
      ],
      "metadata": {
        "id": "kLeG8OVsAJmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1.   Michal Golovanevsky, Carsten Eickhoff, Ritambhara Singh, Multimodal attention-based deep learning for Alzheimer’s disease diagnosis, Journal of the American Medical Informatics Association, Volume 29, Issue 12, December 2022, Pages 2014–2022, https://doi.org/10.1093/jamia/ocac168\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    }
  ]
}